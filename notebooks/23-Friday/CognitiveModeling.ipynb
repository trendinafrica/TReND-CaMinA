{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9F--0OQ148u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Human Behavior"
      ],
      "metadata": {
        "id": "nXQPXqzc5ESu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this Colab, we will put to practive what we've learned in today's lecture:\n",
        "* **We will first train an RL agent to perform a task, using policy evaluation and policy improvement.**\n",
        "* **After that, we will fit an RL model to a real human dataset, and analyze (fake) fMRI data!**\n",
        "\n",
        "To get started, let's first load our dataset to get it out of the way. Execute the following cell to load the dataset from gitbub into this Colab."
      ],
      "metadata": {
        "id": "luGSOAB1fKcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_data = pd.read_csv(\"./bahrami_100.csv\")"
      ],
      "metadata": {
        "id": "bF4mqCksgcTn"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's understand our task. We are working with a \"4-armed bandit task\". The figure below depicts what is happening on each trial of this task from participants' perspective:\n",
        "\n",
        "<center><img src=\"https://github.com/trendinafrica/Comp_Neuro-ML_course/blob/main/notebooks/23-Friday/TaskOverview.png?raw=1\" width=1000></center>\n",
        "\n",
        "## Exercise 1 (*5 minutes*)\n",
        "\n",
        "* Find a partner (turn to your neighbor)\n",
        "* Together, understand the task design:\n",
        "  * First, let the person sitting left explain the first two stages (\"Participant choice\" and \"Chosen stimulus\") to the person sitting right. (*2 minutes*)\n",
        "  * Then, let the person sitting right explain the last two stages (\"Reward\" and \"Inter-trial interval\") to the person sitting left. (*1 minute*)\n",
        "  * Lastly, talk about any questions you still have about this task. (*2 minutes*)"
      ],
      "metadata": {
        "id": "RnF3i-XYCqBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution 1\n",
        "\n",
        "Now, expand the cell below to see the solution."
      ],
      "metadata": {
        "id": "hpTB5-mBkm38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Participants perform the task on a computer. On each trial of the task, participants see four items on the screen (which we sometimes call \"bandits\"). In the \"participant choice\" stage, participants have 4 seconds to pick one of the bandits, using four keys on their keyboard (\"d\", \"f\", \"j\", and \"k\"). Once the participant has made a choice, all bandit except the selected one disappear, and only the selected one stays on the screen for 400 miliseconds (0.4 seconds). Then, the reward is displayed: Participants can win between 1-100 points on each trial, depending on which bandit they choose. The reward stays on the screen for 800 miliseconds, then a fixation cross appears in the center of the screen for half a second. After the fixation cross, participants enter the next trial, which is structured in the same way."
      ],
      "metadata": {
        "id": "giX5CpbtlUWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task, participants do a total of 150 trials, and thereby learn which bandits tend to give more points than others, so they can maximize the points they win. Let's take a look at the dataset!"
      ],
      "metadata": {
        "id": "JN_-QPpBm-s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "T-7uxQlToYt7",
        "outputId": "b4e2dc95-6669-4de5-d7c4-3faafbc91232"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  choice  reward      rt  payoff_group  reward_c1  reward_c2  \\\n",
              "0        1     1.0    84.0  1104.0             2         84         87   \n",
              "1        1     2.0    90.0  1076.0             2         90         90   \n",
              "2        1     3.0    53.0   612.0             2         80         84   \n",
              "3        1     4.0    24.0   742.0             2         87         81   \n",
              "4        1     2.0    92.0   927.0             2         86         92   \n",
              "...    ...     ...     ...     ...           ...        ...        ...   \n",
              "14995  100     3.0    62.0   679.0             2         47         35   \n",
              "14996  100     3.0    61.0   686.0             2         46         47   \n",
              "14997  100     3.0    70.0   600.0             2         46         35   \n",
              "14998  100     3.0    60.0   641.0             2         46         44   \n",
              "14999  100     3.0    56.0   643.0             2         39         35   \n",
              "\n",
              "       reward_c3  reward_c4  \n",
              "0             42         23  \n",
              "1             46         18  \n",
              "2             53         28  \n",
              "3             50         24  \n",
              "4             61         28  \n",
              "...          ...        ...  \n",
              "14995         62         48  \n",
              "14996         61         57  \n",
              "14997         70         43  \n",
              "14998         60         59  \n",
              "14999         56         51  \n",
              "\n",
              "[15000 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c500fed1-be78-4539-bad1-f7fb4875fe6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>choice</th>\n",
              "      <th>reward</th>\n",
              "      <th>rt</th>\n",
              "      <th>payoff_group</th>\n",
              "      <th>reward_c1</th>\n",
              "      <th>reward_c2</th>\n",
              "      <th>reward_c3</th>\n",
              "      <th>reward_c4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>1104.0</td>\n",
              "      <td>2</td>\n",
              "      <td>84</td>\n",
              "      <td>87</td>\n",
              "      <td>42</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>1076.0</td>\n",
              "      <td>2</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>46</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>612.0</td>\n",
              "      <td>2</td>\n",
              "      <td>80</td>\n",
              "      <td>84</td>\n",
              "      <td>53</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>742.0</td>\n",
              "      <td>2</td>\n",
              "      <td>87</td>\n",
              "      <td>81</td>\n",
              "      <td>50</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>927.0</td>\n",
              "      <td>2</td>\n",
              "      <td>86</td>\n",
              "      <td>92</td>\n",
              "      <td>61</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14995</th>\n",
              "      <td>100</td>\n",
              "      <td>3.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>679.0</td>\n",
              "      <td>2</td>\n",
              "      <td>47</td>\n",
              "      <td>35</td>\n",
              "      <td>62</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14996</th>\n",
              "      <td>100</td>\n",
              "      <td>3.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>686.0</td>\n",
              "      <td>2</td>\n",
              "      <td>46</td>\n",
              "      <td>47</td>\n",
              "      <td>61</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14997</th>\n",
              "      <td>100</td>\n",
              "      <td>3.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>600.0</td>\n",
              "      <td>2</td>\n",
              "      <td>46</td>\n",
              "      <td>35</td>\n",
              "      <td>70</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14998</th>\n",
              "      <td>100</td>\n",
              "      <td>3.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>641.0</td>\n",
              "      <td>2</td>\n",
              "      <td>46</td>\n",
              "      <td>44</td>\n",
              "      <td>60</td>\n",
              "      <td>59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14999</th>\n",
              "      <td>100</td>\n",
              "      <td>3.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>643.0</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>35</td>\n",
              "      <td>56</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c500fed1-be78-4539-bad1-f7fb4875fe6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c500fed1-be78-4539-bad1-f7fb4875fe6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c500fed1-be78-4539-bad1-f7fb4875fe6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand this"
      ],
      "metadata": {
        "id": "vlo0fAT5og-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tQHSEk0Soa4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise 2 (10 minutes)\n",
        "\n",
        "* With your partner or by yourself, vizualize how many points each bandit gives on each trial of the task. (You will find the columns `reward_actionX` helpful for this exercise: These columns indicate how much reward each action X would have given on each trial had it been chosen.)\n",
        "* To do this, plot trials (from 0-150) on the x-axis\n",
        "* And plot the reward each arm would have given (from 1-100) on the y-axis\n",
        "* Select a different color for each to distinguish them"
      ],
      "metadata": {
        "id": "LVuv84_-oX1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2rX3q2aOoOmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Let's f"
      ],
      "metadata": {
        "id": "x9SxrepGoMyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to show solution"
      ],
      "metadata": {
        "id": "MHWCZJ9_lJSy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UdLP-zGWlEWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data (100 subjects of 1 reward schedule)\n",
        "# PROVIDE task explanation figure\n",
        "# PROVIDE: time on x-axis, points for each arm on the y-axis -> insight: reward payoffs change over time\n",
        "# TASK 1a: Describe this figure in words. Can you explain the task based on this figure? What would an optimal strategy look like?\n",
        "# TASK 1b: Plot average choices over x-axis\n",
        "# TASK 1c: What do you see? How do you interpret this finding? -> insight: looks similar to plot above! -> people tend to pick some actions over others"
      ],
      "metadata": {
        "id": "JTXcGIea3Z6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Train an RL agent to solve the same task"
      ],
      "metadata": {
        "id": "rsur2SXV7fLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain a bit."
      ],
      "metadata": {
        "id": "1ZOD-dF4Cryi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PROVIDE class for Q-learning agent: random behavioral policy, but calculates Q-values according to Bellman/Q-Update\n",
        "# TASK 2a: Let the agent perform the task (fill in some pieces of code here and there to complete the loop)\n",
        "# TASK 2b: Inspect the behavior (should be random)\n",
        "# TASK 2c: Inspect the value function (should approximate plots above)\n",
        "# TASK 2d: Describe what we have done. (-> Policy Evaluation). What is missing? -> Policy Improvement.\n",
        "# TASK 2e: Implement the policy improvement step (Choose actions according to values)\n",
        "# TASK 2f: Replot behavior (should now look like humans)\n",
        "# TASK 2g: Describe your results in words."
      ],
      "metadata": {
        "id": "PIRB_aIu7lWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Use RL as a model for human behavior"
      ],
      "metadata": {
        "id": "cL2c4Kc_CHfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now trained an RL agent to perform the task. We next want to test if humans might be using RL in a similar way to learn the task. How can we do this?\n",
        "\n",
        "To see if humans use RL to solve the task, we \"fit\" the RL model to human behavior. This means that we \"squeeze\" and \"stretch\" the RL agent until it produces behavior that corresponds to the human behavior.\n",
        "\n",
        "In this case, the \"squeezing\" and \"stretching\" consists of increasing or decreasing the values of the *free parameters* of the model, $\\alpha$ and $\\beta$.\n",
        "\n",
        "How do we know if we need to increase or decrease the values? By checking how close the behavior of the model is to human behavior. The closer the model behavior matches human behavior, the better the model \"fits\" the human dataset. We want the best possible fit, so we are looking for the values of $\\alpha$ and $\\beta$ that *maximizes the probability* that the RL model chooses the same actions that humans have chosen.\n",
        "\n",
        "**In other (more fancy) words, our goal is to find the values for our model parameters ($\\alpha$ and $\\beta$) that maximize the likelihood of the observed (human) behavior under the model.**\n",
        "\n",
        "To do this, we first need to know how likey the observed behavior is under each model. Once we know, all we have to do is to maximize this likelihood.\n",
        "\n",
        "**TASK 1**: Calculate the likelihood of the human dataset under model parameters $\\alpha=0.3$ and $\\beta=3$, by filling in the blanks below.\n",
        "\n",
        "**TASK 2**: Maximize the likelihood of the human dataset by finding the optimal parameters. Fill in the blanks below."
      ],
      "metadata": {
        "id": "oLNfbWE5CtPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same agent, set alpha and beta to the values above. calculate likelihood for one subject.\n",
        "# Write the loss function: negative log likelihood.\n",
        "# Set up a loop that performs SGD on the loss function.\n",
        "# TASK 3: Simulate behavior from the agent with the fitted parameters.\n",
        "# TASK 3b: Plot the behavior like before. Is it closer to humans?"
      ],
      "metadata": {
        "id": "fTSE-belCPFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Use RL as a neural model"
      ],
      "metadata": {
        "id": "CupoYbCnII01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like we've seen in the lecture, there is lots of evidence that the brain might implent an RL algorithm: Most notably, the dopamine system has been argued to calculate reward prediction errors (RPEs), such that dopamine neurons *increase* their firing rates when there is a *positive* RPE (reward is *larger* than expected), and *decrease* their firing rates when there is a *negative* RPE (reward is *smaller* than expected).\n",
        "\n",
        "In this section, we will see if this is the case in our dataset.\n",
        "\n",
        "TASK 1: Calculate RPEs. For each trial in the task, calculate the RPE the model is encountering. (Make sure you save the RPEs for each trial so we can later compare them to human striatal activity.)\n",
        "\n",
        "TASK 2: Plot the model-based RPEs against the human fMRI signal (RPEs on the y-axis and BOLD signal on the x-axis). What do you see?\n",
        "\n",
        "TASK 3: Calcualte the correlation between model-based RPEs and striatal BOLD signal. What do you conclude about the hypothesis that the striatal dopamine systems encodes RPEs?"
      ],
      "metadata": {
        "id": "CsV3ueFVIPsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 4: Calculate RPEs in the model (already done; just need to save)\n",
        "# TASK 4b: Plot\n",
        "# TASK 4c: Calculate correlation"
      ],
      "metadata": {
        "id": "GWZQVLuVIFsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improve the model: Add forgetting"
      ],
      "metadata": {
        "id": "UgYhGu5oKJuY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6AK-gnYKOEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model comparison"
      ],
      "metadata": {
        "id": "Kznmn67cKQWG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLKcc464KSRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus: Fit a neural network to human behavior"
      ],
      "metadata": {
        "id": "RSgCu1hpKA6W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ZMyKwwuKAO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}