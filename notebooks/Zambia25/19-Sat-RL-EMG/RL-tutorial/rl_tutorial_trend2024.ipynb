{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial\n",
    "By Kim Stachenfeld (with substantial contributions from code by Tom George, Jesse Geerts)\n",
    "stachenfeld@deepmind.com\n",
    "\n",
    "## Getting started\n",
    "To run the code in a \"cell\", you'll press Shift-Enter.\n",
    "\n",
    "\n",
    "#### Run locally as a Jupyter Notebook\n",
    "\n",
    "You can open the notebook in jupyter notebook or a locally hosted colab. To run locally, you can open a terminal window and download the code with:\n",
    "\n",
    "```\n",
    "cd /.../TReND-CaMinA/notebooks/Rwanda24/20-Sat-RL\n",
    "jupyter notebook rl_tutorial_trend2024.ipynb\n",
    "```\n",
    "\n",
    "You can also open the notebook in another app that runs python notebooks if you prefer (e.g. VS Studio, [Google colab](https://research.google.com/colaboratory/local-runtimes.html)).\n",
    "\n",
    "### Install + import dependencies\n",
    "\n",
    "This code requires additional dependencies you might not already have installed. \n",
    "\n",
    "The next code cell will install dependencies (this requires an internet connection). You will likely only need to run this once (i.e. once the dependencies are installed locally, you will usually not need to reinstall them when you rerun the code).\n",
    "\n",
    "Dependencies are also listed in `requirements.txt` and can be installed by running `pip install -r requirements.txt` from the Terminal window. If you do this, you can remove or comment out the below cell block (`# Install dependencies`).\n",
    "\n",
    "\n",
    "# Hints\n",
    "\n",
    "**Need help?** To view information about a function or class, type `?` before the name of the function or class. For example, `?print` or `?str` will cause information about the print function or the string datatype to print (in a jupyter notebook) or pop up on the right (in a colab).\n",
    "\n",
    "**Table of Contents:** In colab, you can view the Table of contents for the tutorial, by click the icon in the top left with three dots next to three lines. I do not know of an equivalent in jupyter notebook.\n",
    "\n",
    "# Resources\n",
    "\n",
    "**Code:** Additional code used by the tutorial can be found [in the github page](https://github.com/trendinafrica/TReND-CaMinA/tree/main/notebooks/Rwanda24/20-Sat-RL). You are welcome to peruse it; however, it is not required to understand the tutorial.\n",
    "\n",
    "**Slides:** Slides for the tutorial can be found [here](https://docs.google.com/presentation/d/1P0HxPxZp3JJTv2yEvckjnQXJpBRy4FZW1CWJhNMDJHc/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations, imports, and setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/miniconda3/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (2020.12.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/miniconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.8/site-packages (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/miniconda3/lib/python3.8/site-packages (from networkx) (4.4.2)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: seaborn in /opt/miniconda3/lib/python3.8/site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/miniconda3/lib/python3.8/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/miniconda3/lib/python3.8/site-packages (from seaborn) (1.10.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/miniconda3/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/miniconda3/lib/python3.8/site-packages (from seaborn) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2020.12.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/miniconda3/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2023.3)\n",
      "Requirement already satisfied: six in /opt/miniconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install matplotlib\n",
    "!pip install networkx\n",
    "!pip install numpy\n",
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup\n",
    "\n",
    "**Common errors**\n",
    "\n",
    "`ModuleNotFoundError: No module named 'name_of_some_module'`. This means python cannot find the module.\n",
    "\n",
    "If the module is an external dependency (e.g. `matplotlib`, `networkx`, `numpy`, `seaborn`), it likely means something went wrong with the installation.\n",
    "\n",
    "If the module is an internal dependency (e.g. `env_utils`, `plotting`), it likely means the jupyter notebook command was not run from the same folder as the code. You can rerun jupyter notebook from the directory containing the tutorial code.\n",
    "\n",
    "If this doesn't work, you can also add the directory to Python's path so that it's visible to python by editing and uncommenting the line in the below code cell. This should be a last resort though, as figures will be missing in the documentation.\n",
    "```\n",
    "# sys.path.insert(0, r\"/replace/this/with/path/to/code/and/uncomment\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external dependencies.\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Import internal dependencies.\n",
    "# Uncomment and edit if internal dependencies cannot be found.\n",
    "# sys.path.insert(0, r\"/replace/this/with/path/to/code/and/uncomment\")\n",
    "# import plotting\n",
    "# import env_utils\n",
    "\n",
    "# Set up plotting convenience variables\n",
    "COLORS = sns.color_palette('tab10')\n",
    "BLUE, ORANGE, GREEN, RED, PURPLE, BROWN, PINK, GRAY, LIME, CYAN = COLORS\n",
    "\n",
    "# Set random seed.\n",
    "np.random.seed(0)  # Set random seed for reproducibility\n",
    "\n",
    "# Uncomment this code if you are going to be editing the internal dependencies (plotting.py, env_utils.py)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external dependencies.\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Import internal dependencies.\n",
    "# Uncomment and edit if internal dependencies cannot be found.\n",
    "# sys.path.insert(0, r\"/replace/this/with/path/to/code/and/uncomment\")\n",
    "# import plotting\n",
    "# import env_utils\n",
    "\n",
    "# Set up plotting convenience variables\n",
    "COLORS = sns.color_palette('tab10')\n",
    "BLUE, ORANGE, GREEN, RED, PURPLE, BROWN, PINK, GRAY, LIME, CYAN = COLORS\n",
    "\n",
    "# Set random seed.\n",
    "np.random.seed(0)  # Set random seed for reproducibility\n",
    "\n",
    "# Uncomment this code if you are going to be editing the internal dependencies (plotting.py, env_utils.py)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Env_utils Code\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def list_actions(transitions_dict):\n",
    "  return sorted(np.unique([a for _, a in transitions_dict.keys()]))\n",
    "\n",
    "\n",
    "def list_states(transitions_dict):\n",
    "  return sorted(np.unique([a for s, a in transitions_dict.keys()] + list(transitions_dict.values())))\n",
    "\n",
    "\n",
    "def transition_dict_to_networkx(transition_dict):\n",
    "  states = list_states(transition_dict)\n",
    "  n_states = len(states)\n",
    "  g = nx.DiGraph()\n",
    "  for (s, a), s_next in transition_dict.items():\n",
    "    g.add_edge(s, s_next, action=a)\n",
    "  return g\n",
    "\n",
    "\n",
    "def map_to_array(map, map_to_arr_dict = None):\n",
    "  rows = [row for row in map.split('\\n') if row != '']\n",
    "  n = len(rows)\n",
    "  m = len(rows[0])\n",
    "  arr = np.zeros((n, m), dtype=str)\n",
    "  for i, row in enumerate(rows):\n",
    "    for j, val in enumerate(row):\n",
    "      arr[i, j] = val\n",
    "  return arr\n",
    "\n",
    "\n",
    "def get_coord_to_index_dict(n, m):\n",
    "  coords = np.array(np.meshgrid(range(n), range(m))).reshape(2, -1).T\n",
    "  return {(ii, jj): kk for kk, (ii, jj) in enumerate(coords)}\n",
    "\n",
    "\n",
    "def get_index_to_coord_dict(n, m):\n",
    "  coords = np.array(np.meshgrid(range(n), range(m))).reshape(2, -1).T\n",
    "  return {kk: (ii, jj) for kk, (ii, jj) in enumerate(coords)}\n",
    "\n",
    "\n",
    "def index_to_coord(k, n):\n",
    "  i = k // n\n",
    "  j = k % n\n",
    "  return i, j\n",
    "\n",
    "\n",
    "def arr_to_transition_dict(\n",
    "    arr, start_symbol = 'S', boundary_symbol = 'X', reward_symbols = ['G', 'P'],\n",
    "    reward_amounts = {'G': 1., 'P': -1.}):\n",
    "\n",
    "  transitions = {}\n",
    "  reward_probabilities = {}\n",
    "\n",
    "  coords_to_inds = get_coord_to_index_dict(*arr.shape)\n",
    "  inds_to_coords = get_index_to_coord_dict(*arr.shape)\n",
    "  start_state = 0\n",
    "  n_states = np.max(list(inds_to_coords.keys())) + 1\n",
    "\n",
    "  pos = np.zeros((n_states, 2), dtype=int)\n",
    "\n",
    "  inds = np.array(np.meshgrid(range(arr.shape[0]), range(arr.shape[1]))).reshape(2, -1).T\n",
    "  for state, (x, y) in inds_to_coords.items():\n",
    "    pos[state] = [y, arr.shape[0]-x-1]\n",
    "\n",
    "    symbol = arr[x, y]\n",
    "    if symbol in reward_symbols:\n",
    "      reward_probabilities[state] = reward_amounts[symbol]\n",
    "    else:\n",
    "      reward_probabilities[state] = 0.\n",
    "\n",
    "    if symbol == start_symbol:\n",
    "      start_state = state\n",
    "\n",
    "    # Get adjacent states coords.\n",
    "    possible_next_state_coords = [(x+1, y), (x, y+1), (x-1, y), (x, y-1)]\n",
    "\n",
    "    for action, (xx, yy) in enumerate(possible_next_state_coords):\n",
    "      next_x, next_y = (xx, yy)\n",
    "\n",
    "      # If current state is on a boundary, no external transitions\n",
    "      # are possible -- all actions lead to stay.\n",
    "      if symbol == boundary_symbol:\n",
    "        next_x, next_y = (x, y)\n",
    "\n",
    "      # If action leads you to a next_state that is out of bounds, \n",
    "      # replace next_state with staying at same state.\n",
    "      if ((next_x < 0) or (next_x >= arr.shape[0]) or\n",
    "          (next_y < 0) or (next_y >= arr.shape[1])):\n",
    "        next_x, next_y = (x, y)\n",
    "\n",
    "      # If next_state is an obstacle or wall, replace next state with current.\n",
    "      if arr[next_x, next_y] == boundary_symbol:\n",
    "        next_x, next_y = (x, y)\n",
    "\n",
    "      next_state = coords_to_inds[(next_x, next_y)]\n",
    "      transitions[(state, action)] = next_state\n",
    "\n",
    "\n",
    "  return transitions, reward_probabilities, start_state, pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) refers to learning from rewards. For example, if you reward a dog for doing a trick by rewarding it with a treat whenever it performs the desired behavior, you are training the dog with RL. The RL problem is the problem of finding actions, through trial-and-error, that maximize future reward. RL can be applied to lots of different types of problems, with different available actions and differently defined rewards:\n",
    "\n",
    "<center><img src=\"./figures/rl_examples.png\" width=500></center>\n",
    "\n",
    "The defining aspect of reinforcement learning problems is that the agent is learning from rewards. After the agent takes an action, it is not told whether or not it took the correct action, or what the correct action would have been (\"supervised learning\"). Instead it is given a reward: maybe it gets a reward of +1, or +10, or -10 for an action, and then has to figure out whether or not a more rewarding action exists by exploring its environment. This makes reinforcement learning really hard, especially in variable environments with sparse rewards.\n",
    "\n",
    "*A minor point about terminology.* As a term, Reinforcement Learning sometimes has multiple meanings. It is sometimes used to refer to a set of problems -- whenever an agent or animal is learning about rewards with trial-and-error -- and other times used to refer to a set of solutions -- algorithms specifically developed to *solve* RL problems, like Q-Learning or SARSA (which we'll learn more about later). I think this often leads to confusing when encountering RL for the first time, especially in neuroscience, as the first interpretation describes a set of experiments and the second a specific hypotheses about how the brain might be solving them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent <> Environment\n",
    "\n",
    "The key components of an RL setup are the  `Agent` and `Environment`. These components interact in a loop. The `Environment` takes in `actions` from the agent, updates its internal state, then returns a `reward` and a `observation` from the updated environment to the agent. The `Agent` takes in the `observation` and the recent `reward`, updates its internal variables, and outputs a new `action`. These stages repeat in a loop indefinitely (or until the episode ends).\n",
    "\n",
    "<center><img src=\"./figures/agent_env_loop.png\" width=500></center>\n",
    "\n",
    "Below are two example \"base classes\" outlining the structure of an Agent and an Environment (abbreviated as \"Env\"). Future Agents we set up will inherit this structure. The functions are empty right now meaning that these agents won't do anything (`pass` is filler code that doesn't do anything -- you'll fill these functions in later). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseAgent\n",
    "\n",
    "* `__init__`: this sets up parameters for the agent (such as how quickly it learns).\n",
    "* `update`: this function takes in a reward and observation (outputted by the environment). We have also set it up to take in the previous action that the agent produced. In some experiments, we won't have observations or actions, but we will always have rewards.\n",
    "* `select_action`: this function is used to select an action that will be fed to the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "  \"\"\"Base Agent class.\"\"\"\n",
    "\n",
    "  def __init__(self, name: str = 'RL Agent'):\n",
    "    # Initialize agent state\n",
    "    self.name = name\n",
    "\n",
    "  def update(self, reward, action=None, observation=None):\n",
    "    \"\"\"Update agent state given reward, action, and observation.\n",
    "    \n",
    "    Args:\n",
    "      reward: scalar, reward received (in RL, this is what the\n",
    "        agent is usually trying to maximize)\n",
    "      action: (optional) action taken by the agent that led to that observation and reward.\n",
    "      observation: (optional) observation outputted by the\n",
    "        environment\n",
    "    \"\"\"\n",
    "    # You will fill this in for agents that we'll make later.\n",
    "    # Update agent_state.\n",
    "    # return action\n",
    "    pass\n",
    "\n",
    "  def select_action(self, observation=None):\n",
    "    \"\"\"Select next action given agent state and current observation.\"\"\"\n",
    "    # You will fill this in for agents that we'll make later.\n",
    "\n",
    "    # Select action based on observation and agent state.\n",
    "    # return action\n",
    "    pass\n",
    "\n",
    "  # You will define some params and state variables for each agent, e.g.\n",
    "  variables_to_track = []  # This is a list of names of variables you'll track for analysis\n",
    "  history = {}  # This will contain tracked variables over time\n",
    "\n",
    "  def get_history_to_plot(self):\n",
    "    \"\"\"Get variables saved in history so we can plot them.\"\"\"\n",
    "    return [(k, np.array(self.history[k]).copy()) for k in self.variables_to_track]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseEnv\n",
    "\n",
    "* `__init__`: this sets up parameters for the environment (such as reward probabilities).\n",
    "* `step`: this function takes in an action, updates the internal state of the environment (e.g. moving north one step if the action was \"move north\"), and outputs a new observation (e.g. what the agent sees in its new location) and reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEnv:\n",
    "  \"\"\"Base Environment class.\"\"\"\n",
    "\n",
    "  def __init__(self, name: str = \"Environment\"):\n",
    "    # Initialize environment state (if applicable).\n",
    "    self.name = name\n",
    "\n",
    "  def step(self, action=None):\n",
    "    \"\"\"Update environment state given agent's action and output reward, observations.\n",
    "\n",
    "    Args:\n",
    "      action: action taken by the agent\n",
    "\n",
    "    Returns:\n",
    "      reward: scalar reward.\n",
    "      observation: observation given new environment state\n",
    "    \"\"\"\n",
    "    # You will fill this in for environments that we'll make later.\n",
    "\n",
    "    # Update environment state.\n",
    "    # return reward, observation\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Take a moment to inspect the agent, and familiarize yourself with `Agent` and `Env` classes. Take turns describing them to a neighbor, and ask questions to ensure they make sense.\n",
    "2. Imagine you were setting up an `Agent` and an `Environment` to model different tasks. What types of variables might comprise `action`, `observation`, and `reward`?\n",
    "    * Example: train an agent to play a video game like Pacman\n",
    "        * `action` = click or move joystick, `observation` = screen, `reward` = collect points\n",
    "    * get a mouse to turn left in a maze\n",
    "        * `action` = ?, `observation` = ?, `reward` = ?\n",
    "    * get users to click on advertisements\n",
    "        * `action` = ?, `observation` = ?, `reward` = ?\n",
    "    * train a robot to make paperclips\n",
    "        * `action` = ?, `observation` = ?, `reward` = ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building towards RL\n",
    "\n",
    "In the rest of the tutorial, we'll explore some different RL algorithms and environments. We'll build in ingredients as follows:\n",
    "\n",
    "* **Making predictions about reward:** Pavlovian conditioning and Rescorla-Wagner model\n",
    "* **Chosing actions that maximize reward:** Instrumental learning, n-armed bandits, and the explore-exploit tradeoff\n",
    "* **Foresight:** Maximizing total future reward in Markov Decision Problems, TD Learning, and Online+Offline learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Expectation, and Pavlov's Dogs\n",
    "\n",
    "A famous experiment in the history of psychology was performed by Pavlov on dogs. The type of learning depicted below is called \"Pavlovian Conditioning.\"\n",
    "\n",
    "<center><img src=\"./figures/pavlov.png\" width=300></center>\n",
    "\n",
    "Pavlov first observed that fresh meat -- which is rewarding to dogs -- caused his dogs to salivate. This is an \"unconditioned stimulus\" -- no training is required to product that beahvior. What Pavlov did was to pair fresh meat with a neutral \"conditioned stimulus\" (a bell or whistle). This sound ordinarily does not cause dogs to salivate. What Pavlov did was to ring the bell right before giving the dogs meat. Eventually, he found that just ringing the bell was sufficient to cause the dogs to salivate, presumably because they learned to associate it with reward.\n",
    "\n",
    "Below we have set up a class that implements a probabilistic `PavlovsEnvironment`: whenever Pavlov \"rings a bell\", we will call env.step() and get a reward.\n",
    "\n",
    "### Mini Exercise\n",
    "Inspect the environment in the cell below. Two cells down, you will set up the environment with different reward_amount values. What happens if you change that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PavlovsEnvironment(BaseEnv):\n",
    "  \"\"\"Environment returns probabilistic reward.\"\"\"\n",
    "  def __init__(self, reward_amount: float = 1.):\n",
    "    \"\"\"Initialize.\n",
    "    \n",
    "    Args:\n",
    "      reward_amount: amount of reward delivered.\n",
    "    \"\"\"\n",
    "\n",
    "    # Save reward_amount so that other functions in the class can refer \n",
    "    # to it as self.reward_amount\n",
    "    self.reward_amount = reward_amount\n",
    "\n",
    "    # Iinitializes the super class (aka it calls BaseEnv.__init__() to\n",
    "    # inherit structure from that class)\n",
    "    super().__init__(name='PavlovsEnvironment')\n",
    "\n",
    "  def step(self):\n",
    "    # Pavlov rang the bell!\n",
    "    # You get a treat.\n",
    "    reward = self.reward_amount\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = 10\n"
     ]
    }
   ],
   "source": [
    "reward_amount = 10  # *Exercise*: what happens if you change this value?\n",
    "n_steps = 8\n",
    "env = PavlovsEnvironment(reward_amount)\n",
    "reward = env.step()\n",
    "print(f'Reward = {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking\n",
    "The result of Pavlov's experiment was the observation that animals can learn associations between stimuli (e.g. bell) and reward (e.g. meat) when the two happen at the same time. \n",
    "\n",
    "However, it turns out, it is  not just co-occurence that drives learning: *expectations matter*. This was illustrated with blocking paradigm:\n",
    "\n",
    "<center><img src=\"./figures/blocking.png\" width=400></center>\n",
    "\n",
    "In Phase 1, Stimulus A (a tone) is shown with reward (cheese): A -> R\n",
    "\n",
    "Next, in Phase 2, Stimulus A (a tone) and stimulus B (a light) are shown together with reward (cheese): AB -> R\n",
    "\n",
    "In the Test Phase, we ask if there is a conditioned response to B. In other words, does the animal learn to associate B with reward?\n",
    "\n",
    "#### Exercise\n",
    "Let's vote! What do you think happens? (Don't scroll down -- there are spoilers).\n",
    "1. The animal does learn an association between B and R\n",
    "2. The animal does not learn an association between B and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescorla-Wagner Model. \n",
    "\n",
    "Turns out it is not just co-occurence that drives learning: *expectations matter*. If the reward is already associated with A, then it's not unexpected when it follows AB: it's already entirely predicted by A. So no further learning occurs for B.\n",
    "\n",
    "This motivates a model where it is the **prediction error** -- the difference between the reward that the agent predicts and the reward that it receives that drives learning. \n",
    "\n",
    "Rescorla and Wagner came up with a mathematical model to describe error-driven learning, where the agent's estimate of value $v$ of a stimulus $s$ is updated with a prediction error:\n",
    "\n",
    "$$\n",
    "v \\leftarrow v + \\alpha \\delta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = r - v\n",
    "$$\n",
    "\n",
    "* $v$: expected reward associated with a stimulus.\n",
    "* $r$: reward that was experienced.\n",
    "* $\\delta$: the prediction error $r-v$, the difference between the actual reward $r$ and expected reward $v$.\n",
    "* $\\alpha$: learning rate between 0 and 1 that scales how much value is updated following reward.\n",
    "\n",
    "### Exercise\n",
    "In the below cell, you will write the update function for a Rescorla-Wagner agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagnerAgent(BaseAgent):\n",
    "  \"\"\"Agent that implements Rescorla-Wagner learning.\"\"\"\n",
    "\n",
    "  def __init__(self, v_init: float = 0., learning_rate: float = 0.1, name: str = 'Rescorla Wagner'):\n",
    "    \"\"\"Initialize agent.\n",
    "\n",
    "    Args:\n",
    "      v_init: scalar, initial expected reward (default=0)\n",
    "      learning_rate: float in [0, 1] range; determines how quickly you\n",
    "        update expectations given reward. If 0, expectations will not\n",
    "        be updated. If 1, expectations will be overwritten by most recent\n",
    "        reward (default = 0.1)\n",
    "    \"\"\"\n",
    "    # Check learning rate is in the valid range.\n",
    "    if (learning_rate < 0) or (learning_rate > 1):\n",
    "      raise ValueError(f'learning_rate must be between 0 and 1 (inclusive).')\n",
    "\n",
    "    # Save parameters.\n",
    "    self.alpha = learning_rate\n",
    "\n",
    "    # Intialize v.\n",
    "    self.v = v_init\n",
    "\n",
    "    # Initialize variables to track.\n",
    "    self.variables_to_track = ['Reward', 'Expected Reward', 'Prediction Error']\n",
    "    self.history = {k: [] for k in self.variables_to_track}\n",
    "    super().__init__(name=name)\n",
    "\n",
    "  def update(self, reward):\n",
    "    \"\"\"Update reward expectations using prediction error learning, V = V + alpha * (R-V),\n",
    "      to learn V, the expected reward associated with the observed stimulus.\n",
    "\n",
    "    Args:\n",
    "      reward: the actually observed reward from the environment (R)\n",
    "    \"\"\"\n",
    "    # Keep track of variables before update.\n",
    "    self.history['Expected Reward'].append(self.v)\n",
    "    self.history['Reward'].append(reward)\n",
    "\n",
    "    # *Exercise*: Compute prediction error.\n",
    "    prediction_error = ...\n",
    "\n",
    "    # *Exercise*: Update expected reward, self.v, using prediction error (delta).\n",
    "    self.v += ...\n",
    "\n",
    "    self.prediction_error = prediction_error\n",
    "\n",
    "    # Keep track of variables after update.\n",
    "    self.history['Prediction Error'].append(prediction_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises.\n",
    "In the cell below, perform the following exercises.\n",
    "1. First, make sure your agent runs without error.\n",
    "2. What is the effect on `agent.v` after the update if you change `learning_rate` to a larger or smaller value? What about to extreme values like 0 or 1? What is the effect on `agent.prediction_error`?\n",
    "3. What is the effect on `agent.prediction_error` if you change `v_init` to 1? to 2?\n",
    "4. What happens to prediction_error if you call agent.update(reward=1) a second (or third, or fourth) time?\n",
    "\n",
    "Is there anything else we should do to make sure the agent is running correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'float' and 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e592cabe006f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRescorlaWagnerAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'agent.v: {agent.v}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'agent.prediction_error: {agent.prediction_error}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-cdd0f96445a4>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, reward)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# *Exercise*: Update expected reward, self.v, using prediction error (delta).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'float' and 'ellipsis'"
     ]
    }
   ],
   "source": [
    "agent = RescorlaWagnerAgent(v_init=0., learning_rate=0.1)\n",
    "agent.update(reward=1)\n",
    "print(f'agent.v: {agent.v}')\n",
    "print(f'agent.prediction_error: {agent.prediction_error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pavlovian_learning(env, agent, n_steps: int):\n",
    "  \"\"\"Run Pavlovian conditioning RL loop. \n",
    "  \n",
    "  Args:\n",
    "    env: environment object\n",
    "    agent: agent object\n",
    "    n_steps: number of steps\n",
    "  \n",
    "  Returns:\n",
    "    data from agent for plotting\n",
    "  \"\"\"\n",
    "  for _ in range(n_steps):\n",
    "    reward = env.step()\n",
    "    agent.update(reward)\n",
    "  return agent.get_history_to_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "learning_rate = 0.1\n",
    "reward_amount = 1.\n",
    "v_init = 0.\n",
    "\n",
    "env = PavlovsEnvironment(reward_amount)\n",
    "agent = RescorlaWagnerAgent(v_init=v_init, learning_rate=learning_rate)\n",
    "\n",
    "data_to_plot = run_pavlovian_learning(env, agent, n_steps)\n",
    "\n",
    "_ = plot_learning_variables_over_time(data_to_plot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. What happens to the Expected Reward $v$ over time? What about the prediction error $\\delta$?\n",
    "2. What happens if you change learning_rate to 0? 1? 0.01?\n",
    "3. What happens if you change v_init to 1? 2?\n",
    "4. *Thought experiment:* what would happen if reward was probabilistic (ie you got reward 50% of the time)? What would expected reward be? Would prediction errors ever go down to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about multiple stimuli\n",
    "\n",
    "We motivated Rescorla-Wagner agent with blocking. Let's see if blocking really works. This means we have to extending the agent we defined above so it can learn about multiple stimuli at the same time: instead of learning a single association $v$ for expected reward, we'll learn multiple associations $w_i$ which each convey the expected reward associated with some different stimulus $s_i$ (such as light, tone, smell, taste, etc).\n",
    "\n",
    "The new update for $w_i$ will look like this:\n",
    "\n",
    "$$\n",
    "w_i \\leftarrow w_i + \\alpha s_i \\delta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = r - \\sum_i w_i s_i\n",
    "$$\n",
    "\n",
    "* $w_i$: expected reward associated with each stimulus $i$.\n",
    "* $s_i$: stimulus\n",
    "* $r$: reward that was experienced.\n",
    "* $\\sum_i w_i s_i$: the expected reward, computed by multiplying all present stimuli $s_i$ by their associations $w_i$ and summing them up.\n",
    "* $\\delta$: the prediction error $r-\\sum_i w_i s_i$, the difference between the actual reward $r$ and expected reward $v$.\n",
    "* $\\alpha$: learning rate between 0 and 1 that scales how much value is updated following reward.\n",
    "\n",
    "#### Exercise\n",
    "In the below cell, you will write the update function for a Rescorla-Wagner agent that takes in multiple stimuli. Fill in the lines of code marked `*Exercise*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescorlaWagnerAgentMultipleStim(BaseAgent):\n",
    "  \"\"\"Agent that implements Rescorla-Wagner learning over multiple stimuli simultaneously.\n",
    "  \n",
    "  Instead of keeping track of a single variable, V, the expected reward, this agent will \n",
    "  keep track of multiple weights, w_i, each corresponding to a different stimulus dimension\n",
    "  (e.g. light and sound). Each w_i is updated track the expected reward associated with each\n",
    "  stimulus dimension. Expected reward is computed by multiplying each stimulus present by \n",
    "  its corresponding weight then summing to get total expected reward.\n",
    "  \"\"\"\n",
    "  def __init__(self, n_stim: int, w_init: float, learning_rate: float = 0.1,\n",
    "               name: str = 'Rescorla Wagner Multi Stim'):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      n_stim: the number of stimulus dimensions to learn associations for\n",
    "      w_init: float containing initial association between a stimulus dimension and reward\n",
    "      learning_rate: float, determines how quickly you update expectations (alpha)\n",
    "    \"\"\"\n",
    "    # Save params.\n",
    "    self.n_stim = n_stim\n",
    "    self.w = np.array([w_init] * n_stim)  # an array of length n_stim containing w_init\n",
    "    self.alpha = learning_rate\n",
    "\n",
    "    # Save variables to track.\n",
    "    self.variables_to_track = [\n",
    "      'Reward', 'Total Expected Reward', 'Prediction Error','Expected Reward per Stim', 'Stimuli']\n",
    "    self.history = {k: [] for k in self.variables_to_track}\n",
    "    super().__init__(name=name)\n",
    "\n",
    "  def update(self, reward, stimuli):\n",
    "    \"\"\"Function that implements R-W update for observations containing multiple stimuli.\n",
    "\n",
    "    Args:\n",
    "      reward: scalar, the actually observed reward from the environment (R)\n",
    "      expected_reward_per_stimulus:  array with length n_stimuli containing the\n",
    "        expected reward associated with each stimulus feature\n",
    "      stimuli: array with length n_stimuli containing stimuli present on this trial\n",
    "    \"\"\"\n",
    "    # Check that there are as many stimulus dimensions as association weights to update.\n",
    "    assert len(self.w) == len(stimuli)\n",
    "\n",
    "    # Keep track of variables before update.\n",
    "    self.history['Expected Reward per Stim'].append(self.w.copy())\n",
    "    self.history['Reward'].append(reward)\n",
    "    self.history['Stimuli'].append(stimuli)\n",
    "\n",
    "    # *Exercise*.\n",
    "    # Compute expected reward.\n",
    "    v = ...\n",
    "\n",
    "    # Update expected reward, self.v, using prediction error (aka delta).\n",
    "    prediction_error = ...\n",
    "\n",
    "    # Update association weights, self.w\n",
    "    self.w = ...\n",
    "\n",
    "    self.prediction_error = prediction_error\n",
    "\n",
    "    # Keep track of variables after update.\n",
    "    self.history['Total Expected Reward'].append(v)\n",
    "    self.history['Prediction Error'].append(prediction_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stim = 2\n",
    "learning_rate = 0.1\n",
    "w_init = 0.\n",
    "stim = np.array([1, 0])  # First stimulus is present, second is not.\n",
    "\n",
    "agent = RescorlaWagnerAgentMultipleStim(n_stim, w_init, learning_rate)\n",
    "\n",
    "print(f'Initial agent.w: {agent.w}')\n",
    "\n",
    "agent.update(1, stim)\n",
    "\n",
    "print(f'agent.w: {agent.w}')\n",
    "print(f'agent.prediction_error: {agent.prediction_error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. Check that your agent runs without error. \n",
    "2. Which entries in `agent.w` were updated? What happens if you change `stim` to `np.array([0, 1])` or `np.array([1, 1])`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Environment\n",
    "We'll start exploring our agent in a simple two-stimulus environment with probabilistic reward. The environment randomly samples continuous values for two stimuli from a Gaussian.\n",
    "\n",
    "$s_1, s_2 \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "It then computes reward as:\n",
    "\n",
    "$r = v_1s_1 + v_2s_2 + \\epsilon$\n",
    "\n",
    "where $v_1, v_2$ are stimulus values conveyed in `reward_per_stim`, and $\\epsilon$ is randomly sampled from a Gaussian $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$ where $\\sigma$ is `reward_noise`.\n",
    "\n",
    "We will use this environment to test the agent we wrote in the above cell. We have defined this for you below -- have a look, but we don't need to fill in any code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStimEnv(BaseEnv):\n",
    "  \"\"\"Environment that samples Gaussian stimuli and returns probabilistic reward.\"\"\"\n",
    "\n",
    "  def __init__(self, reward_per_stim: np.ndarray, reward_noise: Optional[float] = None):\n",
    "    \"\"\"Initialize.\n",
    "    \n",
    "    Args:\n",
    "      reward_per_stim: array that has length n_stim with average reward per stimulus feature\n",
    "      reward_noise: the noise added to the sampled rewards.\n",
    "    \"\"\"\n",
    "    self.reward_per_stim = np.array(reward_per_stim)\n",
    "    self.reward_noise = reward_noise if reward_noise else np.mean(np.abs(self.reward_per_stim))/10\n",
    "    self.n_stim = len(reward_per_stim)\n",
    "    super().__init__()\n",
    "\n",
    "  def step(self):\n",
    "    # Sample a random set of positive stimulus values, each from a squared unit Gaussian.\n",
    "    stimuli = np.random.randn(self.n_stim)\n",
    "\n",
    "    # Compute the reward for this stimulus\n",
    "    reward = np.sum(self.reward_per_stim * stimuli)\n",
    "\n",
    "    # Add a little bit of noise (if self.reward_noise > 0)\n",
    "    noise = np.random.randn() * self.reward_noise\n",
    "    reward += noise\n",
    "\n",
    "    return reward, stimuli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another function for setting up an RL loop (what's different compared to `run_pavlovian_agent`?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pavlovian_multistim_agent(env, agent, n_steps):\n",
    "  for tstep in range(n_steps):\n",
    "    reward, stimuli = env.step()\n",
    "    agent.update(reward, stimuli)\n",
    "  return agent.get_history_to_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "learning_rate = .1\n",
    "reward_per_stim = np.array([1., -0.5])\n",
    "reward_noise = 0.\n",
    "n_stim = len(reward_per_stim)\n",
    "\n",
    "env = TwoStimEnv(reward_per_stim, reward_noise=reward_noise)\n",
    "agent = RescorlaWagnerAgentMultipleStim(n_stim=n_stim, w_init=0., learning_rate=learning_rate)\n",
    "\n",
    "data_to_plot = run_pavlovian_multistim_agent(env, agent, n_steps)\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=True, add_labels=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True reward_per_stim: {reward_per_stim}')\n",
    "print(f'Learned agent.w: {agent.w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. Compare `reward_per_stim` to `agent.w`. Are they similar? Do you expect them to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocking\n",
    "\n",
    "We're now ready to try a blocking \"experiment\" on our agent!\n",
    "\n",
    "#### Exercise\n",
    "In the below cell, you'll create an Environment that runs a blocking experiment by filling in the `step()` function in the below cell.\n",
    "\n",
    "Recall the structure of a blocking experiment:\n",
    "1. For `n_steps_A`, the agent sees A alone (`np.array([1, 0])`), followed by reward\n",
    "2. After that, the agent sees both A and B (`np.array([1, 1])`), followed by reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingEnv(BaseEnv):\n",
    "  \"\"\"A single stimulus is paired with the US (A --> R), then a compound stimulus is paired with the US (AB --> R).\"\"\"\n",
    "  def __init__(self, n_steps_A: int = 100):\n",
    "    \"\"\"Initialize.\n",
    "    \n",
    "    Args:\n",
    "      n_steps_A: the number of trials in which A is seen alone, followed by reward.\n",
    "    \"\"\"\n",
    "    self.n_steps_A = n_steps_A\n",
    "    self.n_stim = 2\n",
    "    self.n_steps_elapsed = 0\n",
    "    super().__init__()\n",
    "\n",
    "  def step(self):\n",
    "    # If tstep is less than the number of scheduled exposures to A, only A is present.\n",
    "    # Otherwise, A and B are both present.\n",
    "\n",
    "    # *Exercise:*\n",
    "    # stimuli = ...\n",
    "    # reward = ...\n",
    "\n",
    "    # *Solution*\n",
    "    if self.n_steps_elapsed < self.n_steps_A:\n",
    "      stimuli = np.array([1, 0]) # Just stim A.\n",
    "    else:\n",
    "      stimuli = np.array([1, 1])  # Stim A and Stim B.\n",
    "    \n",
    "    # Reward is always 1.\n",
    "    reward = 1.\n",
    "\n",
    "    # Increment number of elapsed steps by 1.\n",
    "    self.n_steps_elapsed += 1\n",
    "\n",
    "    return reward, stimuli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure your environment runs and is returning sensible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_A = 100\n",
    "env = BlockingEnv(n_steps_A=n_steps_A)\n",
    "reward, stim = env.step()\n",
    "print(f'reward: {reward}')\n",
    "print(f'stim: {stim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_A = 100\n",
    "n_steps_AB = 100\n",
    "n_steps = n_steps_A + n_steps_AB\n",
    "learning_rate = 0.1\n",
    "n_stim = len(reward_per_stim)\n",
    "\n",
    "env = BlockingEnv(n_steps_A=n_steps_A)\n",
    "agent = RescorlaWagnerAgentMultipleStim(n_stim=n_stim, w_init=0., learning_rate=learning_rate)\n",
    "\n",
    "data_to_plot = run_pavlovian_multistim_agent(env, agent, n_steps)\n",
    "fig, axes = plot_learning_variables_over_time(\n",
    "  data_to_plot, show_smooth=False, add_labels=True, stim_labels=['Stim A', 'Stim B'])\n",
    "add_vline(axes, n_steps_A, '--', color=GRAY)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f'agent.w: {agent.w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Inspect the Stimuli plot (far right). What happens at x=n_steps_A on the plot?\n",
    "2. Look at Expected Reward per Stim plot, and look at agent.w. Did any learning happen for Stim B?\n",
    "3. What happens if you change n_steps_A to a smaller number? How does that effect Prediction Error and Expected Reward per Stim plots? What does that suggest about blocking?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional exercises.\n",
    "Rescorla-Wagner models makes a number of other predictions about how learning can be altered. If there is time, start implementing some and see if you can capture some of these classic effects with your agent. These can have the same form as BlockingEnv. Solutions can be found in the solutions.ipynb notebook if you would like to experiment with these tasks later.\n",
    "\n",
    "* `OvershadowingEnv`: Environment that returns two stimuli, by default one large (salient) and one small (not salient).\n",
    "* `OverExpectationEnv`: Two stimuli are seperately paired with the US (A --> R, B --> R), then the compound stimulus is presented (AB --> ?)\n",
    "* `ConditionedInhibition`: A single stimulus is paired with the US (A --> R) then a second stimulus is added and the reward is removed. (AB --> _).\n",
    "\n",
    "For each of these environments, (1) pay attention to the far right plot (Stimuli) to confirm your experiment is working as expected. Then look at (2) what expectations converge to (3) how total expected reward changes and stimulus-wise expectations over time, particularly at events and (4) how prediction errors change over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverShadowingEnv(BaseEnv):\n",
    "  \"\"\"Environment that returns two stimuli, by default one large (salient) and one small (not salient).\"\"\"\n",
    "  def __init__(self, stimuli: np.ndarray = np.array([1., 0.1])):\n",
    "    self.stimuli = stimuli\n",
    "    self.n_stim = 2\n",
    "    self.n_steps_elapsed = 0\n",
    "    super().__init__()\n",
    "\n",
    "  def step(self):\n",
    "    # *Optional Exercise*\n",
    "    ...\n",
    "    return reward, stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "stimuli = np.array([1., 0.1])\n",
    "learning_rate = 0.1\n",
    "n_stim = len(stimuli)\n",
    "\n",
    "env = OverShadowingEnv(stimuli)\n",
    "agent = RescorlaWagnerAgentMultipleStim(n_stim=n_stim, w_init=0., learning_rate=learning_rate)\n",
    "\n",
    "data_to_plot = run_pavlovian_multistim_agent(env, agent, n_steps)\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=False, add_labels=True, stim_labels=['Stim A', 'Stim B'])  \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Try different values for the two stimuli. How does this effect the expected reward per stim?\n",
    "\n",
    "Do these values make sense?\n",
    "\n",
    "Are there alternate values the agent could have assigned to these stimuli that would have given equally valid predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverExpectation(BaseEnv):\n",
    "  \"\"\"Two stimuli are seperately paired with the US (A --> R, B --> R), then the compound stimulus is presented (AB --> ?).\"\"\"\n",
    "  def __init__(self, n_steps_separate: int = 100):\n",
    "    self.n_steps_separate = n_steps_separate\n",
    "    self.n_stim = 2\n",
    "    self.n_steps_elapsed = 0\n",
    "    super().__init__()\n",
    "\n",
    "  def step(self):\n",
    "    # *Optional Exercise*\n",
    "    ...\n",
    "    return reward, stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_separate = 100\n",
    "n_steps = n_steps_separate + 10\n",
    "learning_rate = 0.1\n",
    "n_stim = 2\n",
    "w_init = 0.\n",
    "\n",
    "env = OverExpectation(n_steps_separate)\n",
    "agent = RescorlaWagnerAgentMultipleStim(n_stim=n_stim, w_init=w_init, learning_rate=learning_rate)\n",
    "\n",
    "data_to_plot = run_pavlovian_multistim_agent(env, agent, n_steps)\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=False, add_labels=True, stim_labels=['Stim A', 'Stim B'])\n",
    "add_vline(axes, n_steps_separate, '--', color=GRAY)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedInhibition(BaseEnv):\n",
    "  \"\"\"A single stimulus is paired with the US (A --> R) then a second stimulus is added and the reward is removed. (AB --> _).\"\"\"\n",
    "  def __init__(self, n_steps_A: int = 100):\n",
    "    self.n_steps_A = n_steps_A\n",
    "    self.n_stim = 2\n",
    "    self.n_steps_elapsed = 0\n",
    "    super().__init__()\n",
    "\n",
    "  def step(self):\n",
    "    # *Optional Exercise*\n",
    "    ...\n",
    "    return reward, stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_A = 100\n",
    "n_steps_AB = 100\n",
    "n_steps = n_steps_A + n_steps_AB\n",
    "learning_rate = 0.1\n",
    "n_stim = 2\n",
    "w_init = 0.\n",
    "\n",
    "env = ConditionedInhibition(n_steps_A=n_steps_A)\n",
    "agent = RescorlaWagnerAgentMultipleStim(n_stim=n_stim, w_init=w_init, learning_rate=learning_rate)\n",
    "\n",
    "data_to_plot = run_pavlovian_multistim_agent(env, agent, n_steps)\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=False, add_labels=True, stim_labels=['Stim A', 'Stim B'])\n",
    "add_vline(axes, n_steps_A, '--', color=GRAY)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making Good Choices\n",
    "\n",
    "\n",
    "Section 1 covered expectations: how to learn to predict reward (which is an important part of learning how to maximize it). But, in the Pavlovian conditioning examples above, the agent does not actually get to make any choices, or take actions to *affect* reward. And of course that is something agents and animals can usually do (otherwise, there is arguably not much of a point to being able to predict anything). We'll now cover Instrumental Learning\", where, in contrast to Pavlovian Conditioning, covers experiments where animals (or agents) can make choices.\n",
    "\n",
    "We'll study this in the context of a task called the $n$-armed bandit tasks. This comes from a nickname for slot machines (since they have an \"arm\" you can pull, and they steal your money like a bandit).\n",
    "\n",
    "Say you are at a casino and there are $n$ \"arms\" available to you. \n",
    "\n",
    "<center><img src=\"./figures/bandits.png\" width=500></center>\n",
    "\n",
    "Each arm delivers reward with some probability, which you don't know. The only way to learn about the rewards is to try them. \n",
    "\n",
    "What strategy do you use to sample arms, given that you want to learn as much as you can about the different arms, while also not wasting time on suboptimal arms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic reward.\n",
    "The experiments we will work with in the following section will involve probabilistic reward. We will define a function `sample_reward` to sample binary rewards (ie rewards that are either 0 or 1) with a specified probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reward(prob):\n",
    "  \"\"\"Sample from a binomial (weighted coin flip) with probability prob.\"\"\"\n",
    "  if (prob < 0) or (prob > 1):\n",
    "    raise ValueError(f'Probability must remain between 0 and 1. prob={prob} not valid.')\n",
    "  random_number = np.random.rand()  # Sample random number between 0 and 1\n",
    "  if random_number < prob:  # return 1 with probability = prob\n",
    "    return 1\n",
    "  else:                     # otherwise return 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_sample_reward():\n",
    "  prob = 0.5  # Play around with different values here.\n",
    "  n_samples = 10  # Play around with different values here.\n",
    "  rewards = []\n",
    "  for i in range(n_samples):\n",
    "    reward = sample_reward(prob)\n",
    "    print(f'Trial {i} reward = {reward}')\n",
    "    rewards.append(reward)\n",
    "  mean_reward = np.mean(rewards)\n",
    "  print(f'Mean reward = {mean_reward}')\n",
    "\n",
    "test_sample_reward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Experiment by setting different `prob` and `n_samples`. What is the relationship between `prob`, `n_samples`, and `mean_reward`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Armed Bandit\n",
    "Here, we set up an n-armed bandit environment using sample_reward function. This environment takes in an array of reward probabilities with lenght equal to the number of arms, such that `reward_probabilities[action]` returns the probability that the chosen action will lead to reward.\n",
    "\n",
    "The `step` function of this environment will take in an action (aka a choice in \"arm\"), compute the reward probability for the action, and then sample a reward with the probability.\n",
    "\n",
    "#### Exercise\n",
    "You will fill in the missing code in `NArmedBanditEnv` to select the correct probability and sample a reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NArmedBanditEnv(BaseEnv):\n",
    "  \"\"\"N-armed bandit environment.\"\"\"\n",
    "\n",
    "  def __init__(self, reward_probabilities: np.ndarray):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      reward_probabilities: an array that has length n_arms, with values\n",
    "        between 0 and 1 (inclusive). It specifies the probability of reward\n",
    "        for each action.\n",
    "    \"\"\"\n",
    "    self.reward_probabilities = np.array(reward_probabilities)\n",
    "    self.n_actions = len(reward_probabilities)\n",
    "\n",
    "    if len(self.reward_probabilities.shape) > 1:\n",
    "       rp_shape = self.reward_probabilities.shape\n",
    "       raise ValueError(\n",
    "          f'reward_probabilities should be a one dimensional array. Found shape={rp_shape}')\n",
    "\n",
    "  def step(self, action: int):\n",
    "    \"\"\"Get reward for the arm the agent selected.\n",
    "\n",
    "    Args:\n",
    "      action: an integer specifying which an action (aka which arm) was\n",
    "        chosen by the agent.\n",
    "\n",
    "    Returns:\n",
    "      reward: an integer (either 0 or 1) specifying the received reward.\n",
    "    \"\"\"\n",
    "    if action > self.n_actions:\n",
    "        raise ValueError(f'Action {action} is greater than the number of arms {self.n_actions}')\n",
    "\n",
    "    # *Exercise*\n",
    "    reward_probability = ...\n",
    "    reward = ...\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. Confirm your environment runs without error.\n",
    "2. Experiment with different `rewardprobabilities` and different values for `n_arms` to confirm the environment is working as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 2\n",
    "n_steps = 5\n",
    "reward_probabilities = np.array([0.1, 0.9])\n",
    "env = NArmedBanditEnv(reward_probabilities)\n",
    "\n",
    "for _ in range(n_steps):\n",
    "  reward = env.step(action=0)\n",
    "  print(f'action = 0, reward = {reward}')\n",
    "print()\n",
    "for _ in range(n_steps):\n",
    "  reward = env.step(action=1)\n",
    "  print(f'action = 1, reward = {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Now we turn our attention to the agent. We'll implement a learning algorithm called \"Q-learning\".\n",
    "\n",
    "Q-learning involves learning the expected reward associated with each action. It's called Q-learning because it involves learning Q-values, which refers to the expected reward associated with an action (I do not know why they are called Q-values). \n",
    "\n",
    "So notationally, $Q(a)$ is the expected reward associated with an action.\n",
    "\n",
    "#### Updating Q\n",
    "We learn Q-values with the same error-driven learning we used in Rescorla-Wagner, but now it is conditional on action $a$. So instead of $v(s) \\leftarrow v(s) + \\alpha \\delta$, we'll have...\n",
    "\n",
    "$Q(a) \\leftarrow Q(a) + \\alpha \\delta(a)$\n",
    "\n",
    "$\\delta(a) = r - Q(a)$\n",
    "\n",
    "Where $a$ is action, $Q(a)$ is the expected value of $a$, $r$ is reward, $\\alpha$ is learning rate, and $\\delta$ refers to the prediction error. \n",
    "\n",
    "#### Choosing actions\n",
    "For now, we'll choose actions with a \"greedy policy\", which means we will just pick whichever action has the largest q-value associated with it. This will not turn out to be a very good idea, but we'll get to that.\n",
    "\n",
    "\n",
    "#### Exercise\n",
    "Below, we've defined a function `greedy_choice` that takes in an array of q-values, and returns the action with the highest q-value (or, if there is a tie, it will randomly sample one of the best). Below that is an agent called `GreedyQAgent`, which uses greedy_choice to an action. \n",
    "\n",
    "Fill in the lines of code marked `*Exercise*` in the update function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_choice(q: np.ndarray):\n",
    "  \"\"\"Return an action with the maximum estimated reward probability.\"\"\"\n",
    "  # Get action(s) with maximum q.\n",
    "  q_max = np.max(q)\n",
    "  best_actions = np.argwhere(q == q_max).reshape(-1)\n",
    "\n",
    "  # Sample randomly from any top actions.\n",
    "  action = np.random.choice(best_actions)\n",
    "  return action\n",
    "\n",
    "\n",
    "class GreedyQAgent(BaseAgent):\n",
    "  \"\"\"Q-learning agent that always selects the most rewarding action.\"\"\"\n",
    "\n",
    "  def __init__(self, n_actions: int, q_init: float, learning_rate: float,\n",
    "               name: str = 'Greedy Q-Learning'):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      n_actions: number of actions available to the agent\n",
    "      q_init: float indicating the initial q-value associated with each action.\n",
    "      learning_rate: learning rate with which q-values are updated.\n",
    "    \"\"\"\n",
    "    self.n_actions = n_actions\n",
    "    self.q = np.array([q_init] * n_actions, dtype=float)\n",
    "    self.alpha = learning_rate\n",
    "    self.variables_to_track = ['Reward', 'Prediction Error', 'Q-Values', 'Action']\n",
    "    self.history = {k: [] for k in self.variables_to_track}\n",
    "    super().__init__(name=name)\n",
    "\n",
    "  def update(self, reward, action):\n",
    "    \"\"\"Get reward for the arm the agent selected.\n",
    "\n",
    "    Args:\n",
    "      reward: an integer (either 0 or 1) specifying the received reward.\n",
    "      action: an integer specifying which an action (aka which arm) was\n",
    "        chosen by the agent.\n",
    "    \"\"\"\n",
    "    if action > self.n_actions:\n",
    "        raise ValueError(f'Action {action} is greater than the number of arms {self.n_actions}')\n",
    "\n",
    "    # Keep track of variables before update.\n",
    "    self.history['Q-Values'].append(self.q.copy())\n",
    "    self.history['Reward'].append(reward)\n",
    "    self.history['Action'].append(action)\n",
    "\n",
    "    # *Exercise*\n",
    "    # Get expected reward.\n",
    "    v = ...\n",
    "\n",
    "    # Update expected reward, self.v, using prediction error (aka delta).\n",
    "    prediction_error = ...\n",
    "\n",
    "    # Update association weights, self.w\n",
    "    self.q[action] += ...\n",
    "\n",
    "    self.prediction_error = prediction_error\n",
    "\n",
    "    # Keep track of variables after update.\n",
    "    self.history['Prediction Error'].append(prediction_error)\n",
    "\n",
    "  def choose_action(self):\n",
    "    \"\"\"Return the action with the highest estimated reward probability.\"\"\"\n",
    "    return greedy_choice(self.q)\n",
    "\n",
    "  def total_reward(self):\n",
    "    return np.sum(self.history['Reward'])\n",
    "\n",
    "  def rate_of_reward(self):\n",
    "    return np.mean(self.history['Reward'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Confirm that your agent runs without error, and that it picks the most rewarding arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 3\n",
    "q_init = 0.\n",
    "learning_rate = 0.1\n",
    "agent = GreedyQAgent(n_actions, q_init, learning_rate)\n",
    "action = agent.choose_action()\n",
    "reward = 1.\n",
    "agent.update(reward, action)\n",
    "print(f'agent.q: {agent.q}')\n",
    "print(f'agent.prediction_error: {agent.prediction_error}')\n",
    "\n",
    "# Force agent.q to be [0, 1, 0] and confirm that it selects the most rewarding action (action=1)\n",
    "forced_best_action = 1\n",
    "agent.q = np.zeros(n_actions)\n",
    "agent.q[forced_best_action] = 1\n",
    "action = agent.choose_action()\n",
    "print(f'action={action} (should be {forced_best_action}, otherwise there is likely an error)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an RL learning loop with the agent taking in rewards and outputting actions, and the environment taking in actions and outputting rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qagent_bandit(env, agent, n_steps):\n",
    "\n",
    "  for tstep in range(n_steps):\n",
    "    action = agent.choose_action()  # Agent chooses an action\n",
    "    reward = env.step(action)  # Environment offers an action\n",
    "    agent.update(reward, action)  # Agent updates its expectations\n",
    "\n",
    "  return agent.get_history_to_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how our agent does on a 6-armed bandit task with `reward_probabilities = [0, 0.2, 0.4, 0.6, 0.8, 1.0]`\n",
    "\n",
    "Note that the \"worst\" action here is action =  (the first choice in the list), and the \"best\" action here is action=5 (the last choice in the list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "reward_probabilities = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "n_arms = len(reward_probabilities)\n",
    "q_init = 0.\n",
    "agent = GreedyQAgent(n_arms, q_init, learning_rate=0.1)\n",
    "env = NArmedBanditEnv(reward_probabilities)\n",
    "\n",
    "data_to_plot = run_qagent_bandit(env, agent, n_steps)\n",
    "\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=False, add_labels=True, \n",
    "                                                       stim_labels=[f'Arm {i}' for i in range(n_arms)])\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'estimated best_arm: {np.argmax(agent.q)}')\n",
    "print(f'Total reward: {agent.total_reward()} (max possible={n_steps})')\n",
    "print(f'Rate of reward: {agent.rate_of_reward()} (max possible=1)')\n",
    "\n",
    "counts, _ = np.histogram(agent.history['Action'], bins=range(agent.n_actions))\n",
    "counts = {a: c for a, c in enumerate(counts)}\n",
    "print(f'Action counts: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Which arm does your agent converge on as the \"best arm\"? \n",
    "2. Look at the \"Action\" plot. Did your agent explore more than one arm?\n",
    "3. Try running the above cell a few times -- does the agent always pick the same \"best arm\"? Does it usually get the correct \"best arm\" (5)? Does it ever get the worst arm (0)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore-Exploit Tradeoff\n",
    "\n",
    "Likely what you saw in the above cell is that the agent stayed with the first arm that gave any reward. This is because after that first experience with reward, the updated Q-value for the chosen action is incremented to be larger than all the other values. Since the agent is always picking the action it thinks is best, it never explores the other actions. If it did, it would eventually learn that there are better arms out there.\n",
    "\n",
    "This illustrates a fundamental tension in RL: the Explore-Exploit tradeoff. If you have a policy of only picking the action with the highest current estimated Q-value, you lose an opportunity to get information about other outcomes. But if you waste too much time sampling actions that don't have a high estimated value, you are wasting actions you could be spending cashing in on more rewarding arms. Such is life.\n",
    "\n",
    "\n",
    "### Optimistic Initialization\n",
    "We're going to learn some strategies for dealing with this. First, we're going to return to the above cell and try a simple strategy called **optimistic initialization.** Optimistic initialization means picking an optimistic value for `q_init`: a value higher than you're likely to actually experience most of the time. This means that more experience with the environment will likely lower your q-values (unless the action truly is really good). This gives an artificial bonus to actions that have not been explored. \n",
    "\n",
    "#### Exercise\n",
    "* Return to the above cell, and chance q_init to 1. \n",
    "* Does your agent find the correct best_action now?\n",
    "* How do the learned Q-values compare to the true values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy Q-learning\n",
    "Another strategy for managing the explore-exploit tradeoff is to decide to choose a random action sometimes. $\\epsilon$-greedy means that with probability $\\epsilon$, you randomly chose an action. Otherwise, you pick the best action.\n",
    "\n",
    "#### Exercise\n",
    "Below is an agent `EpsilonGreedyQAgent` based on the `GreedyQAgent`. You will modify `choose_action` to implement the $\\epsilon$-greedy action selection policy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyQAgent(GreedyQAgent):\n",
    "  \"\"\"Q-learning agent that always selects the most rewarding action.\"\"\"\n",
    "\n",
    "  def __init__(self, n_actions: int, q_init: float, learning_rate: float, epsilon: float,\n",
    "               name: str = 'Epsilon Greedy Q-Learning'):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      n_actions: integer indicating the number of available actions.\n",
    "      q_init: float indicating the initial q-value associated with each action.\n",
    "      learning_rate: learning rate with which q-values are updated.\n",
    "      epsilon: float indicating probability of taking a random exploratory action\n",
    "    \"\"\"\n",
    "    super().__init__(n_actions=n_actions, q_init=q_init, learning_rate=learning_rate,\n",
    "                     name=name)\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "  def choose_action(self):\n",
    "    \"\"\"Return the action with an epsilon-greedy policy.\"\"\"\n",
    "    # # *Exercise*\n",
    "    # aciton = ...\n",
    "    # return action\n",
    "\n",
    "    # *Solution*\n",
    "    do_explore = np.random.rand() < self.epsilon\n",
    "\n",
    "    # Explore: select random action.\n",
    "    if do_explore:\n",
    "      action = np.random.randint(self.n_actions)\n",
    "\n",
    "    # \"Exploit\": Select greedy action\n",
    "    else:\n",
    "      action = greedy_choice(self.q)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. Run the below cell to confirm the agent is working.\n",
    "2. Test your code with epsilon=0, epsilon=1, and epsilons in between. When epsilon is zero, you should always chose action 0. As epsilon approaches 1, action choice should become more random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.\n",
    "agent = EpsilonGreedyQAgent(n_actions=2, q_init=0., learning_rate=0.1, epsilon=epsilon)\n",
    "\n",
    "# Update the value of action 0 so it is the highest.\n",
    "action = 0\n",
    "agent.update(reward=1, action=action)\n",
    "agent.prediction_error\n",
    "action = agent.choose_action()\n",
    "print(f'action: {action} (should be action={action} if epsilon is 0, and 100% random if epsilon is 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Now we'll revist the 6-armed bandit problem (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "reward_probabilities = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "n_arms = len(reward_probabilities)\n",
    "q_init = 0.\n",
    "epsilon = 0.1\n",
    "agent = EpsilonGreedyQAgent(n_arms, q_init, learning_rate=0.1, epsilon=epsilon)\n",
    "env = NArmedBanditEnv(reward_probabilities)\n",
    "\n",
    "\n",
    "data_to_plot = run_qagent_bandit(env, agent, n_steps)\n",
    "\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=False, add_labels=True, \n",
    "                                                       stim_labels=[f'Arm {i}' for i in range(n_arms)])\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'estimated best_arm: {np.argmax(agent.q)}')\n",
    "print(f'Total reward: {agent.total_reward()} (max possible={n_steps})')\n",
    "print(f'Rate of reward: {agent.rate_of_reward()} (max possible=1)')\n",
    "\n",
    "counts, _ = np.histogram(agent.history['Action'], bins=range(agent.n_actions))\n",
    "counts = {a: c for a, c in enumerate(counts)}\n",
    "print(f'Action counts: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Does the agent sample a variety of actions, and eventually find the correct action? How does total reward compare to GreedyQAgent?\n",
    "2. Experiment with different values of epsilon [0, 0.01, .1, 1.].\n",
    "    * Which values of epsilon allow q-values to converge faster: larger or smaller values? \n",
    "    * Which values lead to the most total reward: 0, 1, or something in-between? \n",
    "    * When $\\epsilon$ is 0, does the agent behave any differently from `GreedyQAgent`?\n",
    "3. (Thought experiment) How would you find the optimal epsilon? Is there a single optimal epsilon, or does it depend on the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Exploration\n",
    "Finally, we'll explore one more popular exploration strategy: softmax-greedy exploration. Like $\\epsilon$-greedy, softmax-greedy will sometimes chose actions not currently estimated to be optimal. $\\epsilon$-greedy will randomly pick from all sub-optimal actions when it is exploring: it makes no distinction between the second-best and worst action.\n",
    "\n",
    "Softmax greedy samples actios with a probability that increases with the q-value of the action, so that exploratory actions prioritize actions that are also estimated to high value. The softmax equation is:\n",
    "\n",
    "$$\n",
    "p(a) = \\frac{e^{\\beta q(a)}}{\\sum_a e^{\\beta q(a)}}\n",
    "$$\n",
    "\n",
    "where $a$ is action, $q(a)$ is the expected action value, $\\beta$ is an exploration parameter called the inverse temperature parameter (terminology is borrowed from physics). Below is an illustration of action probabilities for different values of $\\beta$ and $\\epsilon$.\n",
    "\n",
    "<center><img src=\"./figures/epsilon_v_softmax.png\" width=500></center>\n",
    "\n",
    "Smaller values for $\\beta$ mean more exploration (for $\\beta=0$, p(a) is uniform same for all actions, just like when $\\epsilon=1$). As $\\beta$ grows to infinity, the agent will approach a greedy policy (just like when $\\epsilon=0$). For in-between values, softmax and $\\epsilon$-greedy differ: softmax varies smoothly.\n",
    "\n",
    "\n",
    "We have implemented the agent below, and will explore its behavior in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  \"\"\"Applies the softmax function.\"\"\"\n",
    "  return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def sample_from_multinomial(probs):\n",
    "  \"\"\"Samples from a probability distribution.\"\"\"\n",
    "  return int(np.random.choice(range(len(probs)), size=(), p=probs))\n",
    "\n",
    "\n",
    "class SoftmaxGreedyQAgent(GreedyQAgent):\n",
    "  \"\"\"Q-learning agent that always selects the most rewarding action.\"\"\"\n",
    "\n",
    "  def __init__(self, n_actions: int, q_init: float, learning_rate: float, beta: float,\n",
    "               name: str = 'Softmax Greedy Q Agent'):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      n_actions: integer specifying number of actions available.\n",
    "      q_init: an array that has length n_arms, which has the initial\n",
    "        estimates for the value of each arm.\n",
    "      learning_rate: learning rate with which q-values are updated.\n",
    "      beta: float indicating the softmax inverse temperature parameter.\n",
    "    \"\"\"\n",
    "    super().__init__(n_actions=n_actions, q_init=q_init, learning_rate=learning_rate,\n",
    "                     name=name)\n",
    "    self.beta = beta\n",
    "\n",
    "  def choose_action(self):\n",
    "    \"\"\"Return the action according to a softmax-greedy policy\"\"\"\n",
    "    choice_probs = softmax(self.beta * self.q)\n",
    "    action = sample_from_multinomial(choice_probs)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "reward_probabilities = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "n_arms = len(reward_probabilities)\n",
    "q_init = 0.\n",
    "agent = SoftmaxGreedyQAgent(n_actions=n_arms, q_init=q_init, learning_rate=0.1, beta=0)\n",
    "action = agent.choose_action()\n",
    "env = NArmedBanditEnv(reward_probabilities)\n",
    "\n",
    "\n",
    "data_to_plot = run_qagent_bandit(env, agent, n_steps)\n",
    "\n",
    "fig, axes = plot_learning_variables_over_time(data_to_plot, show_smooth=False, add_labels=True, \n",
    "                                                       stim_labels=[f'Arm {i}' for i in range(n_arms)])\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'estimated best_arm: {np.argmax(agent.q)}')\n",
    "print(f'Total reward: {agent.total_reward()} (max possible={n_steps})')\n",
    "print(f'Rate of reward: {agent.rate_of_reward()} (max possible=1)')\n",
    "\n",
    "counts, _ = np.histogram(agent.history['Action'], bins=range(agent.n_actions))\n",
    "counts = {a: c for a, c in enumerate(counts)}\n",
    "print(f'Action counts: {counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "How do the chosen actions (particularly the Action Counts) and learned q-values learned by the softmax greedy policies differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Foresight\n",
    "## Incorporating future rewards into our plans.\n",
    "\n",
    "Many decisions are about optimizing for future reward, rather than immediate reward. For instance, deciding to submit an application for a summer school is accepting short term discomfort (writing an application) in exchange for long-term reward (the thrill of learning about computational neuroscience!).\n",
    "\n",
    "The final thing we consider will be how to adapt the RL process to include future expected reward.\n",
    "\n",
    "This is typically modelled with Markov Decision Processes (MDPs), which capture an environment that has structure. An MDP contains multiple states, and actions influence not just reward but how the agent can transition through this environment.\n",
    "\n",
    "Here we illustrate an MDP with three states, and 2 actions that move the agent between states. \n",
    "\n",
    "<center><img src=\"./figures/mdp.png\" width=500></center>\n",
    "\n",
    "\n",
    "This environment is linear: the agent can transition from state $s_n$ can transition to state $s_{n+1}$ with action $a=0$ and $s_{n-1}$ with action $a=1$ (except at the boundaries, where the agent stays in place if $s_{n+1}$ or $s_{n-1}$ is out of limits). The agent receives reward at the rightmost state $s_{N-1}$ (given $N$ states). In our experiments, the agent will start at the leftmost state $s_0$.\n",
    "\n",
    "Below, we construct an MDP environment `MDPEnv`. This environment is specified with a reward function (`reward_amounts`), transitions (`transitions`), as well as starting state `start_state` (and, optionally, absorbing states `absorb_states`). Here, we are assuming deterministic transitions, which means that the next state $s'$ is fully determined by current state $s$ and action $a$. \n",
    "\n",
    "We also show an example MDPEnv with 4-states arranged linearly (as in the illustration above) called `FourStateLinearMDPEnv`. We also include `LinearMDPEnv` which sets up linear MDP environments for an arbitrary number of states.\n",
    "\n",
    "#### Exercise\n",
    "Look at how the environments are set up. In particular look at how `FourStateLinearMDPEnv` sets up `transitions` and `reward_amounts`. There are no coding exercises in the next cell, simply an example of how the environment is set up to inspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPEnv(BaseEnv):\n",
    "  \"\"\"N-armed bandit environment.\"\"\"\n",
    "  def __init__(self, reward_amounts, transitions, start_state: int,\n",
    "               absorb_states = None, pos_for_plotting = None):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      reward_amounts: a dict that has length n_states, with values\n",
    "        between 0 and 1 (inclusive). It specifies the amount of reward\n",
    "        received at each state. States not in this dict will default to 0 reward.\n",
    "      transitions: dict containing possible transitions, in the form\n",
    "        (state, action): next_state.\n",
    "      start_state: integer identifying starting state.\n",
    "      absorb_states: array of integers identifying which states are\n",
    "        absorbing (meaning that landing on this state will end the episode).\n",
    "        Default=None, meaning episode will go on forever.\n",
    "      pos_for_plotting: n_states x 2 array with positions for states to be\n",
    "        used for plotting (default = None, will default to line or\n",
    "        spring_layout depending on transition structure).\n",
    "    \"\"\"\n",
    "\n",
    "    self.reward_amounts = reward_amounts\n",
    "\n",
    "    self.transitions = transitions\n",
    "    self.start_state = start_state\n",
    "\n",
    "    if absorb_states is None:\n",
    "      self.absorb_states = []\n",
    "    elif not np.iterable(absorb_states):\n",
    "      self.absorb_states = [absorb_states]\n",
    "    else:\n",
    "      self.absorb_states = absorb_states\n",
    "\n",
    "    self.actions = list_actions(self.transitions)\n",
    "    self.n_actions = len(self.actions)\n",
    "\n",
    "    self.states = list_states(self.transitions)\n",
    "    self.n_states = len(self.states)\n",
    "\n",
    "    # If any states are ommitted from reward_probabilities, fill in with 0.\n",
    "    for k in list(set(self.states) - set(self.reward_amounts.keys())):\n",
    "      self.reward_amounts[k] = 0\n",
    "\n",
    "    self.graph = _dict_to_networkx(self.transitions)\n",
    "    self.pos_for_plotting = nx.spring_layout(self.graph, seed=0) if pos_for_plotting is None else pos_for_plotting\n",
    "\n",
    "    self.state = start_state\n",
    "\n",
    "  def step(self, action: int):\n",
    "    \"\"\"Get reward for the arm the agent selected.\n",
    "\n",
    "    Args:\n",
    "      action: an integer specifying which an action (aka which arm) was\n",
    "        chosen by the agent.\n",
    "\n",
    "    Returns:\n",
    "      reward: an integer (either 0 or 1) specifying the received reward.\n",
    "    \"\"\"\n",
    "    if action > self.n_actions:\n",
    "        raise ValueError(f'Action {action} is greater than the number of arms {self.n_actions}')\n",
    "\n",
    "    end_of_episode = self.state in self.absorb_states\n",
    "    reward = self.reward_amounts[self.state]\n",
    "    next_state = self.transitions[(self.state, action)]\n",
    "\n",
    "    self.state = next_state\n",
    "\n",
    "    return reward, next_state, end_of_episode\n",
    "\n",
    "  def reset(self):\n",
    "    self.state = self.start_state\n",
    "    return self.state\n",
    "\n",
    "  def plot_env(self, ax = None, node_size: Optional[int] = 300, vmin=None, vmax=None):\n",
    "    g = self.graph\n",
    "    pos = self.pos_for_plotting\n",
    "    node_color = np.zeros(self.pos_for_plotting.shape[0])\n",
    "    for s, r in self.reward_amounts.items():\n",
    "      node_color[s] = r\n",
    "    do_labels = self.n_states < 16\n",
    "    vmin = vmin or node_color.min() - 0.2\n",
    "    vmax = vmax or node_color.max() + 0.2\n",
    "    plot_graph(\n",
    "      self.graph, self.pos_for_plotting, ax=ax, node_color=node_color, node_size=node_size,\n",
    "      vmin=vmin, vmax=vmax, cmap='gray_r', do_labels=do_labels)\n",
    "\n",
    "@property\n",
    "def n_states(self):\n",
    "  return self.n_states\n",
    "\n",
    "@property\n",
    "def n_actions(self):\n",
    "  return self.n_actions\n",
    "\n",
    "\n",
    "class FourStateLinearMDPEnv(MDPEnv):\n",
    "  \"\"\"4-state linear MDP env with starting state in left-most state and reward in right-most.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    reward_amounts = {\n",
    "       0: 0,\n",
    "       1: 0,\n",
    "       2: 0,\n",
    "       3: 1,\n",
    "    }\n",
    "\n",
    "    transitions = {\n",
    "        # (state, action): next_state\n",
    "        # Increment state if \"right\" action (action=0) is chosen\n",
    "        (0, 0): 1,\n",
    "        (1, 0): 2,\n",
    "        (2, 0): 3,\n",
    "        (3, 0): 3,\n",
    "        # Decrement state if \"left\" action (action=1) is chosen\n",
    "        (0, 1): 0,\n",
    "        (1, 1): 0,\n",
    "        (2, 1): 1,\n",
    "        (3, 1): 2,\n",
    "    }\n",
    "\n",
    "    pos_for_plotting = np.array([(i, 0) for i in range(4)])\n",
    "\n",
    "    super().__init__(\n",
    "      reward_amounts=reward_amounts,\n",
    "      transitions=transitions,\n",
    "      start_state=0,\n",
    "      absorb_states=None,\n",
    "      pos_for_plotting=pos_for_plotting)\n",
    "\n",
    "\n",
    "\n",
    "class LinearMDPEnv(MDPEnv):\n",
    "  \"\"\"Linear MDP env with starting state in left-most state and reward in right-most.\"\"\"\n",
    "\n",
    "  def __init__(self, n_states: int):\n",
    "    reward_amounts = {n_states-1: 1}\n",
    "\n",
    "    transitions = {}\n",
    "    # Update with transitions to the left.\n",
    "    transitions.update({(s, 0): min(s+1, n_states-1) for s in range(n_states)})\n",
    "    # Update with transitions to the right.\n",
    "    transitions.update({(s, 1): max(s-1, 0) for s in range(n_states)})\n",
    "    pos_for_plotting = np.array([(i, 0) for i in range(n_states)])\n",
    "\n",
    "    super().__init__(\n",
    "      reward_amounts=reward_amounts,\n",
    "      transitions=transitions,\n",
    "      start_state=0,\n",
    "      absorb_states=None,\n",
    "      pos_for_plotting=pos_for_plotting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `FourStateLinearMDPEnv` MDP.\n",
    "\n",
    "* Arrows denote possible transitions between states (action $a=0$ transitions right, $a=1$ transitions left).\n",
    "* the agent receives reward when it reaches the dark gray state\n",
    "* The agent starts in the left-most state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FourStateLinearMDPEnv()\n",
    "env.plot_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Plot the `LinearMDPEnv` MDP in the cell below. Vary `n_states` to see how the environment changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LinearMDPEnv(n_states=10)\n",
    "env.plot_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Exercise\n",
    "If you are ahead of schedule, set up your own MDP, `MyMDPEnv` in the style of `FourStateLinearMDPEnv`. Replace the definitions of `self.transition` and `self.reward_amounts` with your own values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyMDPEnv(MDPEnv):\n",
    "  \"\"\"Make up your own MDP to try with your agents.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    # reward_amounts = {\n",
    "    #    state: reward_amount,\n",
    "    # }\n",
    "\n",
    "    # *Exercise\n",
    "    reward_amounts = ...\n",
    "\n",
    "    # transitions = {\n",
    "    #     # (state, action): next_state\n",
    "    # }\n",
    "\n",
    "    # *Exercise\n",
    "    transitions = ...\n",
    "\n",
    "    pos_for_plotting = np.array([(i, 0) for i in range(4)])\n",
    "\n",
    "    super().__init__(\n",
    "      reward_amounts=reward_amounts,\n",
    "      transitions=transitions,\n",
    "      start_state=0,\n",
    "      absorb_states=None,\n",
    "      pos_for_plotting=pos_for_plotting)\n",
    "\n",
    "\n",
    "# env = MyMDPEnv()\n",
    "# env.plot_env\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Differences\n",
    "\n",
    "MDPs are a framework for setting up environments with sequential structure: the agent's potential reward depends not just on its current state, but on future states it may occupy.\n",
    "\n",
    "We now need to modify our learning algorithm so we are learning to incorporate future outcomes into its valuation. Intuitively, if state A has no reward, but eventually leads to a state B with high reward, state 1 should have some value as well because it *predicts future reward*. \n",
    "\n",
    "Thus we will define the value of a state $V(s)$ to be equal to the expected discounted future reward, where $\\gamma$ is a discount factor between 0 and 1 that determines the planning horizon. A lower discount means the agent cares more about immediate than future reward. \n",
    "\n",
    "$$\n",
    "V(s_t) = r(s_t) + \\gamma r(s_{t+1}) + \\gamma ^2 r(s_{t+2}) + ... = \\sum_{i=0}^\\infty r(s_{t+i})\n",
    "$$\n",
    "\n",
    "We can rearrange the parentheses and write this as...\n",
    "$$\n",
    "V(s_t) = r(s_t) + \\gamma (r(s_{t+1}) + \\gamma r(s_{t+2}) + ...) = r(s_t) + \\gamma \\sum_{i=0}^\\infty r(s_{t+1+i})\n",
    "$$\n",
    "\n",
    "Since the rightmost term is the definition of value for state $s_{t+1}$, we can write our value function recursively! \n",
    "\n",
    "$$\n",
    "V(s_t) = r(s_t) + \\gamma V(s_{t+1})\n",
    "$$\n",
    "\n",
    "This equation is known as the **Bellman Equation.** One of the reasons it is exciting is that it enables us to incorporate future expected reward into error-driven learning. This is not trivial to do in the sequential context: how do we predict an error -- \"actual\" minus \"predicted\" -- when our \"actual\" term reward includes an unlimited sequence of future rewards which we haven't experienced yet? Turns out we can just replace the future reward terms with the current estimate for state value $V_t$. Even though this $V_t$ is something our agent is learning, and will be wrong at first, it will provably eventually converge to the right answer for $V$:\n",
    "\n",
    "$$\n",
    "V_{t+1}(s_t) \\leftarrow V_t(s_t) + \\alpha \\delta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = r(s_t) + \\gamma V_t(s_{t_1}) - V_t(s_t)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dopamine and Reward Prediction Errors\n",
    "\n",
    "In a famous paper, Schultz, Dayan, and Montague (1997) show evidence that the firing of dopamine neurons appears to be consistent with DA neurons encoding a \"reward prediction error\", firing for unexpected but not expected reward.:\n",
    "\n",
    "<center><img src=\"./figures/schultz.png\" width=500></center>\n",
    "\n",
    "This is the origin of the \"Reward Prediction Error Hypothesis\", which hypothesizes that dopamine encodes a reward prediction error that drives reward-driven learning in the cortex. This has spawned lots of work expressing more complex variants of dopaminergic reward driven learning that seek to explain more of the variability and complexity in the dopamine signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning agent.\n",
    "\n",
    "To incorporate action selection into the learning process, we replace state value $V(s)$ with Q-values $Q(s, a)$, which learn the expected value not just of a state but of state-action pairs. We also replace the next-state value term $V_t(s_{t+1})$ with $\\max_a Q(s_{t+1}, a)$. This, intuitively, is making the implicit assumption that the agent will chose the most rewarding next action from state $s_{t+1}$, which is a good assumption if you've got an optimal agent (or are trying to learn one).\n",
    "\n",
    "$$\n",
    "Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\delta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = r(s_t) + \\gamma \\max_a Q_t(s_{t+1}, a) - Q_t(s_t, a_t)\n",
    "$$\n",
    "\n",
    "#### Exercise\n",
    "Implement a q-learning update in the following agent by filling in portions of the code marked `*Exercise*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTDAgent(BaseAgent):\n",
    "  \"\"\"Implement Q-learning agent.\"\"\"\n",
    "\n",
    "  def __init__(self, n_states: int, n_actions: int, q_init: float, learning_rate: float,\n",
    "               epsilon: float, discount: float, name='Q-Learning TD Agent'):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      n_states: number of states\n",
    "      n_actions: number of actions\n",
    "      q_init: float indicating initial value for each state, action pair.\n",
    "      learning_rate: learning rate with which q-values are updated.\n",
    "      epsilon: exploration parameter for epsilon-greedy (randomly \"explore\" with\n",
    "        probability epsilon)\n",
    "      discount: discount factor in [0, 1) range used to scale future reward\n",
    "        (0 = only immediate reward matters, 0.999... = infinite planning horizon)\n",
    "      name: name of agent\n",
    "    \"\"\"\n",
    "    self.n_states = n_states\n",
    "    self.n_actions = n_actions\n",
    "    self.q = np.zeros((n_states, n_actions), dtype=float) + q_init\n",
    "    self.alpha = learning_rate\n",
    "    self.epsilon = epsilon\n",
    "\n",
    "    if (discount < 0) or (discount >=1):\n",
    "      raise ValueError(f'discount={discount} not valid: must be 0 <= discount < 1')\n",
    "\n",
    "    self.gamma = discount\n",
    "    self.variables_to_track = ['Reward', 'Prediction Error', 'State Values', 'Action', 'State', 'Q-Values']\n",
    "    self.history = {k: [] for k in self.variables_to_track}\n",
    "    super().__init__(name=name)\n",
    "  \n",
    "  def compute_prediction_error(self, state, action, reward, next_state, next_action: Optional[int] = None):\n",
    "    # *Exercise*\n",
    "    # Get predicted value\n",
    "    predicted = ...\n",
    "\n",
    "    # Get actual value (reward + gamma * next_state_value)\n",
    "    actual = ...\n",
    "\n",
    "    # Compute the difference\n",
    "    td_error =...\n",
    "\n",
    "    return td_error\n",
    "\n",
    "  def update(self, state, action, reward, next_state, next_action: Optional[int] = None):\n",
    "    \"\"\"Get reward for the arm the agent selected.\n",
    "\n",
    "    Args:\n",
    "      state: an integer specifying which state the agent is in\n",
    "      action: an integer specifying which action was chosen by the agent.\n",
    "      reward: an integer (either 0 or 1) specifying the received reward.\n",
    "      next_state: an integer specifying which state the agent transitioned to next\n",
    "      next_action: an integer specifying which action was chosen by the agent next\n",
    "        (note: this is unused in Q-learning but is accepted to be consistent with\n",
    "        other agents)\n",
    "    \"\"\"\n",
    "\n",
    "    if state > self.n_states:\n",
    "        raise ValueError(f'State {state} is greater than the number of states {self.n_states}')\n",
    "\n",
    "    if action > self.n_actions:\n",
    "        raise ValueError(f'Action {action} is greater than the number of arms {self.n_actions}')\n",
    "\n",
    "    # Keep track of variables before update.\n",
    "    self.history['Q-Values'].append(self.q.copy())\n",
    "    self.history['State Values'].append(self.q.max(1))\n",
    "    self.history['Reward'].append(reward)\n",
    "    self.history['Action'].append(action)\n",
    "    self.history['State'].append(state)\n",
    "\n",
    "    td_error = self.compute_prediction_error(state, action, reward, next_state, next_action)\n",
    "\n",
    "    # Update q[state, action] with prediction error.\n",
    "    self.q[state, action] += self.alpha * td_error\n",
    "\n",
    "    self.prediction_error = td_error\n",
    "\n",
    "    # Keep track of variables after update.\n",
    "    self.history['Prediction Error'].append(td_error)\n",
    "\n",
    "  def choose_action(self, state):\n",
    "    \"\"\"Return the action with the highest estimated reward probability.\"\"\"\n",
    "    do_explore = np.random.rand() < self.epsilon\n",
    "\n",
    "    # Explore: select random action.\n",
    "    if do_explore:\n",
    "      action = np.random.randint(self.n_actions)\n",
    "\n",
    "    # \"Exploit\": Select greedy action\n",
    "    else:\n",
    "      action = greedy_choice(self.q[state])\n",
    "    return action\n",
    "\n",
    "  def count_state_visits(self):\n",
    "    visits, _ = np.histogram(self.history['State'], self.n_states)\n",
    "    return visits\n",
    "  \n",
    "  def total_reward(self):\n",
    "    return np.sum(self.history['Reward'])\n",
    "\n",
    "  def rate_of_reward(self):\n",
    "    return np.mean(self.history['Reward'])\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningTDAgent(n_states=4, n_actions=2, q_init=0, learning_rate=0.1, epsilon=0.2, discount=0.9)\n",
    "print(f'Initial agent.q, {agent.q}')\n",
    "\n",
    "# state 1 -> 2, reward received\n",
    "state = 1\n",
    "action = 0\n",
    "next_state = 2\n",
    "reward_12 = 1\n",
    "\n",
    "prediction_error = agent.compute_prediction_error(\n",
    "  state=state, action=action, reward=reward_12, next_state=next_state)\n",
    "agent.update(state=state, action=action, reward=reward_12, next_state=next_state)\n",
    "print('\\nTransition from state 1 -> 2, R=1')\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'prediction_error {agent.prediction_error}')\n",
    "\n",
    "# state 0 -> 1, no reward received\n",
    "state = 0\n",
    "action = 0\n",
    "next_state = 1\n",
    "reward_01 = 0\n",
    "\n",
    "prediction_error = agent.compute_prediction_error(\n",
    "  state=state, action=action, reward=reward, next_state=next_state)\n",
    "agent.update(state=state, action=action, reward=reward_01, next_state=next_state)\n",
    "print('\\nTransition from state 0 -> 1, R=0')\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'prediction_error {agent.prediction_error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Confirm that the agent runs.\n",
    "2. Confirm that agent.q is updated following the first update for state 1, action 0 following the reward.\n",
    "3. In the following update, no reward is received, but the agent transitions to state 1, which has a positive q value. Does this result in a positive prediction error?\n",
    "4. modify `reward_12` so that it is 0. What effect does that have on the prediction errors that follow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a function to run our TD agent on an MDP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mdp_rl(env, agent, n_steps, n_episodes: Optional[int] = None):\n",
    "  if n_episodes is None:\n",
    "    n_episodes = 1\n",
    "\n",
    "  for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    action = agent.choose_action(state)\n",
    "    for tstep in range(n_steps):\n",
    "      reward, next_state, end_of_episode = env.step(action)\n",
    "      next_action = agent.choose_action(next_state)\n",
    "      agent.update(state, action, reward, next_state, next_action)\n",
    "\n",
    "      state = next_state\n",
    "      action = next_action\n",
    "\n",
    "  return agent.get_history_to_plot()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100  # number of steps in an episode\n",
    "n_episodes = 1  # number of episodes (each new episode starts the agent over at the start state)\n",
    "\n",
    "q_init = 0.\n",
    "learning_rate = 0.1\n",
    "epsilon = 0.1\n",
    "discount = 0.9\n",
    "\n",
    "env = FourStateLinearMDPEnv()\n",
    "agent = QLearningTDAgent(env.n_states, env.n_actions, q_init=q_init, learning_rate=learning_rate, epsilon=epsilon, discount=discount)\n",
    "\n",
    "data_to_plot = run_mdp_rl(env, agent, n_steps, n_episodes=n_episodes)\n",
    "fig, axes = plot_learning_variables_over_time(\n",
    "  data_to_plot, show_smooth=False, add_labels=True, window_size=10,\n",
    "  stim_labels=[f'State {s}' for s in env.states], state_action_q_values=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot environment and visitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "env.plot_env(ax=ax)\n",
    "ax.set_title('Reward')\n",
    "\n",
    "ax = axes[1]\n",
    "agent.count_state_visits()\n",
    "plot_graph(env.graph, env.pos_for_plotting, node_color=agent.q.mean(1),\n",
    "                    cmap='viridis', vmin=None, vmax=None, ax=ax, cbar=True)\n",
    "ax.set_title('Q-values (averaged over actions)')\n",
    "\n",
    "ax = axes[2]\n",
    "agent.count_state_visits()\n",
    "plot_graph(env.graph, env.pos_for_plotting, node_color=agent.count_state_visits(),\n",
    "                    cmap='viridis', vmin=None, vmax=None, ax=ax, cbar=True)\n",
    "ax.set_title('Number of visits to each state')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Which state, action emerges as the most valuable after training? Is that also the state that the agent visits the most?\n",
    "2. What happens if you increase `epsilon` to 1?\n",
    "3. What happens if you increase the number of episodes, `n_episodes`?\n",
    "4. What happens if you decrease `discount` to 0?\n",
    "4. Try replacing `env = FourStateLinearMDPEnv()` with `env = LinearMDPEnv(n_states)`, and vary the number of states. What effect does this have? You may need to increase `n_episodes`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid worlds. \n",
    "Here we will apply the same learning process but to a new and exciting class of MDPs: grid world. A grid world is an environment where states are arranged in a grid, which we will denote with an ascii map like the one below. This allows us to design maps. For example, the below map sets up an environment with a start state at the top left, a goal at the lower right, and a wall in between.\n",
    "\n",
    "```\n",
    "0S00P\n",
    "00000\n",
    "0XXX0\n",
    "000G0\n",
    "```\n",
    "\n",
    "`S` denotes the starting state, `G` the goal, `P` denotes a punishment, `X` denotes an obstacle or boundary with, and `0` denotes states with no reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "n_episodes = 20\n",
    "\n",
    "map = \"\"\"\n",
    "0S000\n",
    "00000\n",
    "00XX0\n",
    "000G0\n",
    "\"\"\"\n",
    "\n",
    "# Specify how much reward (positive or negative) is associated with each symbol (besides 0, S, X)\n",
    "reward_amounts = {'G': 1, 'P': -10}\n",
    "\n",
    "# Set up environment.\n",
    "arr = map_to_array(map)  # convert map to array\n",
    "transitions, reward_amounts, start_state, pos = arr_to_transition_dict(arr, reward_amounts)  # convert array to MDP arguments\n",
    "\n",
    "env = MDPEnv(reward_amounts, transitions=transitions, start_state=start_state,\n",
    "             absorb_states=None, pos_for_plotting=pos)\n",
    "agent = QLearningTDAgent(env.n_states, env.n_actions, 0., learning_rate=0.1, epsilon=0.1, discount=.9)\n",
    "\n",
    "data_to_plot = run_mdp_rl(env, agent, n_steps, n_episodes=n_episodes)\n",
    "fig, axes = plot_learning_variables_over_time(\n",
    "  data_to_plot, show_smooth=True, add_labels=False, window_size=100,\n",
    "  stim_labels=[f'State {s}' for s in env.states], state_action_q_values=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax = axes[0]\n",
    "_, im = plot_grid_heatmap(env.pos_for_plotting, value=agent.q.max(1), ax=ax, cmap='RdBu', center_vaxis=True)\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title('Max State-Values')\n",
    "\n",
    "ax = axes[1]\n",
    "_, im = plot_grid_heatmap(env.pos_for_plotting, value=agent.count_state_visits(), ax=ax, cmap='viridis')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title('State visits')\n",
    "\n",
    "ax = axes[2]\n",
    "plot_graph(env.graph, env.pos_for_plotting, ax=ax)\n",
    "ax.axis('off')\n",
    "ax.set_title('Graph')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. What state does the agent find most rewarding? Is this also the state that the agent visits the most?\n",
    "2. What happens when you increase the discount factor? Increase epsilon?\n",
    "3. Edit the map to generate new gridworlds with different structures. Can you construct an environment that the agent cannot solve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy and On-policy Learning.\n",
    "\n",
    "The final topic we will cover is on-policy versus off-policy learning.\n",
    "\n",
    "Q-learning is an \"off-policy\" algorithm. It uses $\\max_a Q(s_{t+1}, a)$, the maximum *possible* next value, rather than $Q(s_{t+1}, a_{t+1})$, the *actual* value of the observed next state. So essentially we are learning about a policy that is different from the one we are taking. This assumption is ok (intuitively) because the agent is still learning: we assume that eventually, it will be picking the best action.\n",
    "\n",
    "Below, we'll implement an \"on-policy\" algorithm, which instead uses the *actual* next_action $Q(s_{t+1}, a_{t+1})$. This algorithm is called SARSA, which stands for state-action-reward-state-action, because it takes in `state`, `action`, `reward`, `next_state`, `next_action`, whereas Q-learning doesn't need `next_action` to be specified.\n",
    "\n",
    "#### Exercise\n",
    "Below we will implement a SARSA agent with the update:\n",
    "\n",
    "$$\n",
    "Q_{t+1}(s_t, a_t) \\leftarrow Q_t(s_t, a_t) + \\alpha \\delta\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta = r(s_t) + \\gamma Q_t(s_{t+1}, a_{t+1}) - Q_t(s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSATDAgent(QLearningTDAgent):\n",
    "  \"\"\"SARSA Agent \"\"\"\n",
    "\n",
    "  def __init__(self, n_states: int, n_actions: int, q_init: float,\n",
    "               learning_rate: float, epsilon: float, discount: float, \n",
    "               name: str = 'SARSA TD Agent'):\n",
    "    \"\"\"Initialize.\n",
    "\n",
    "    Args:\n",
    "      n_states: number of states\n",
    "      n_actions: number of actions\n",
    "      q_init: initial value for each state, action pair.\n",
    "      learning_rate: learning rate with which q-values are updated.\n",
    "      epsilon: exploration parameter for epsilon-greedy\n",
    "      discount: discount factor in [0, 1) range used to scale future reward\n",
    "        (0 = only immediate reward matters, 0.999... = infinite planning horizon)\n",
    "      name: name of agent class (default='Softmax SARSA TD Agent')\n",
    "    \"\"\"\n",
    "    super().__init__(n_states=n_states, n_actions=n_actions, q_init=q_init, learning_rate=learning_rate,\n",
    "                     epsilon=epsilon, discount=discount, name='Softmax SARSA TD Agent')\n",
    "  \n",
    "  def compute_prediction_error(self, state, action, reward, next_state, next_action):\n",
    "    # *Exercise*\n",
    "    # Get predicted value\n",
    "    predicted = ...\n",
    "\n",
    "    # Get actual value (reward + gamma * next_state_value)\n",
    "    actual = ...\n",
    "\n",
    "    # Compute the difference\n",
    "    td_error = ...\n",
    "\n",
    "    return td_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SARSATDAgent(n_states=4, n_actions=2, q_init=0, learning_rate=0.1, epsilon=0.2, discount=0.9)\n",
    "print(f'Initial agent.q, {agent.q}')\n",
    "\n",
    "# state 1 -> 2, reward received\n",
    "state = 1\n",
    "action = 0\n",
    "next_state = 2\n",
    "next_action = 0\n",
    "reward_12 = 1\n",
    "\n",
    "prediction_error = agent.compute_prediction_error(\n",
    "  state=state, action=action, reward=reward_12, next_state=next_state, next_action=next_action)\n",
    "agent.update(state=state, action=action, reward=reward_12, next_state=next_state, next_action=next_action)\n",
    "print('\\nTransition from state 1 -> 2, R=1')\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'prediction_error {agent.prediction_error}')\n",
    "\n",
    "# state 0 -> 1, no reward received\n",
    "state = 0\n",
    "action = 0\n",
    "next_state = 1\n",
    "next_action_01 = 0\n",
    "reward_01 = 0\n",
    "\n",
    "prediction_error = agent.compute_prediction_error(\n",
    "  state=state, action=action, reward=reward_01, next_state=next_state, next_action=next_action_01)\n",
    "agent.update(state=state, action=action, reward=reward, next_state=next_state, next_action=next_action_01)\n",
    "print('\\nTransition from state 0 -> 1, R=0')\n",
    "print(f'agent.q {agent.q}')\n",
    "print(f'prediction_error {agent.prediction_error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "1. Confirm that the agent runs and is updating itself, and that (for these settings) the SARSA agent does the same thing that the Q-learning agent did (above). \n",
    "2. Change `next_action_01` to 1. What effect does this have on the prediction_error in the 0->1 transition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cliff world\n",
    "Often, SARSA and Q-learning will converge to pretty similar policies. But certain environments can be used to tease them apart. The cliff world is one such example.\n",
    "\n",
    "<center><img src=\"./figures/cliff.png\" width=500></center>\n",
    "\n",
    "The cliff world contains a band of states with extremely low negative reward -- a \"cliff\", which if you go off of, will result in catastrophe. Walking along the cliff is the shortest path to reward, and a \"confident\" agent that assumes it will always take the optimal action and therefore will never walk off the cliff will deem this path optimal. An agent that is more realistic (or pessimistic) may not make this assumption, and might prefer the longer route.\n",
    "\n",
    "Below, we will implement a simple cliff world, and train both of our agents (Q-learning and SARSA) to see how their policies differ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "n_episodes = 200\n",
    "\n",
    "map = \"\"\"\n",
    "0000000000\n",
    "00XXXXXX00\n",
    "0000000000\n",
    "S0PPPPPP0G\n",
    "\"\"\"\n",
    "\n",
    "arr = map_to_array(map)\n",
    "transitions, reward_amounts, start_state, pos = arr_to_transition_dict(\n",
    "    arr, reward_symbols=['P', 'G'], reward_amounts={'P': -100, 'G': 1})\n",
    "\n",
    "learning_rate = 0.1\n",
    "epsilon = 0.1\n",
    "discount=0.8\n",
    "\n",
    "env = MDPEnv(reward_amounts, transitions=transitions, start_state=start_state, absorb_states=None, pos_for_plotting=pos)\n",
    "q_agent = QLearningTDAgent(env.n_states, env.n_actions, q_init=q_init, learning_rate=learning_rate, epsilon=epsilon, discount=discount)\n",
    "sarsa_agent = SARSATDAgent(env.n_states, env.n_actions, q_init=q_init, learning_rate=learning_rate, epsilon=epsilon, discount=discount)\n",
    "\n",
    "agents = [q_agent, sarsa_agent]\n",
    "for agent in agents:\n",
    "  data_to_plot = run_mdp_rl(env, agent, n_steps, n_episodes=n_episodes)\n",
    "  fig, axes = plot_learning_variables_over_time(\n",
    "    data_to_plot, show_smooth=True, add_labels=False, window_size=100,\n",
    "    stim_labels=[f'State {s}' for s in env.states], state_action_q_values=True)\n",
    "  fig.suptitle(f'Agent = {agent.name}')\n",
    "  print(f'{agent.name} Total reward: {agent.total_reward()}')\n",
    "\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = len(agents)\n",
    "fig, axes = plt.subplots(2, n_agents, figsize=(8, 5))\n",
    "\n",
    "for i, agent in enumerate(agents):\n",
    "  ax = axes[0, i]\n",
    "  _, im = plot_grid_heatmap(env.pos_for_plotting, agent.q.max(1), cmap='RdBu', ax=ax, center_vaxis=True)\n",
    "  plt.colorbar(im, ax=ax)\n",
    "  ax.set_title(f'{agent.name}\\nMax State-Values')\n",
    "\n",
    "  ax = axes[1, i]\n",
    "  _, im = plot_grid_heatmap(env.pos_for_plotting, np.log(agent.count_state_visits()+1), cmap='viridis', ax=ax)\n",
    "  plt.colorbar(im, ax=ax)\n",
    "  ax.set_title(f'{agent.name}\\nState Visit count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "1. Which route is preferred by each of the two agents?\n",
    "2. Try increasing epsilon. What effect does this have on the policies?\n",
    "3. Try increasing discount. What effect does this have on the policies?\n",
    "4. Try making the punishment even lower. What happens?\n",
    "5. (thought experiment) Which route is \"better\"? Is that well-defined?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep RL\n",
    "While we have been working with \"tabular\" environments (environments that can be represented as tables with state, action entries), the same general methodology can be applied to neural networks.\n",
    "\n",
    "<center><img src=\"./figures/deeprl.png\" width=500></center>\n",
    "\n",
    "We won't focus on that today, but if you want to get experience with that, you see our tutorial from Cosyne 2023:\n",
    "* [lecture](https://www.youtube.com/watch?v=lJA2QP8ollI)\n",
    "* [code](https://drive.google.com/file/d/1jEiDNA1q98n1Wrw_uBEFvpuV9BGW_yxW/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Safety & Ethics\n",
    "#### Discussion\n",
    "What are some safety issues that arise in RL?\n",
    "* **Alignment:** Think back to our earlier example (\"How would you train a robot to make paperclips\" from the beginning of the tutorial). This is a famous thought experiment called the paperclip problem from Nick Bostrom:\n",
    "<center><img src=\"./figures/paperclip_problem.png\" width=500></center>\n",
    "\n",
    "* **Engagement:** Think back to our other earlier example about getting users to click on advertisements. Similar approaches are applied to get people to keep scrolling on social media, stay engaged in videos, and buy things. What are risks associated with this?\n",
    "\n",
    "* **Safety:** Reinforcement learning works by trial-and-error. What are some settings in the real-world where this would be a bad idea? How would you mitigate it?\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
